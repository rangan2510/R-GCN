{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.0-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 916 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.1)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.0)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.0\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 583 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.5\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 60.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.1)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.5\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (22.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 22.1 MB 53.5 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.5\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 260 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.5.0.tar.gz (153 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 153 kB 2.9 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Collecting plyfile\r\n",
      "  Downloading plyfile-0.7.2-py3-none-any.whl (39 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 11.8 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.1-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 13.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.4.5.1)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.5.0-py3-none-any.whl size=267915 sha256=6c6b263aba5099757a2fc7fd8b23cd03b6812369ef0b43006a41ad95ba3b405a\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/09/84/1d026c4e02fc66af6224faaa7c35db83a178ec6593e69baf06\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: plyfile, isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.1 googledrivedownloader-0.4 isodate-0.6.0 plyfile-0.7.2 rdflib-5.0.0 torch-geometric-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "! pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    log_steps = 1\n",
    "    embedding_dim = 128\n",
    "    walk_length = 80\n",
    "    context_size = 20\n",
    "    walks_per_node = 10\n",
    "    batch_size = 256\n",
    "    num_layers = 3\n",
    "    skip_layers = 3\n",
    "    hidden_channels = 256\n",
    "    dropout = 0.05\n",
    "    lr = 0.01\n",
    "    epochs = 100\n",
    "    eval_steps = 1\n",
    "    runs = 1\n",
    "    use_node_embedding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:06<00:00, 31.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 352.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins')\n",
    "\n",
    "data = dataset[0]\n",
    "\n",
    "model = Node2Vec(data.edge_index, args.embedding_dim, args.walk_length,\n",
    "                     args.context_size, args.walks_per_node,\n",
    "                     sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=args.batch_size, shuffle=True,\n",
    "                      num_workers=4)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(model):\n",
    "    torch.save(model.embedding.weight.data.cpu(), 'embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Step: 100/518, Loss: 4.3751\n",
      "Epoch: 01, Step: 200/518, Loss: 2.0068\n",
      "Epoch: 01, Step: 300/518, Loss: 1.4070\n",
      "Epoch: 01, Step: 400/518, Loss: 1.2177\n",
      "Epoch: 01, Step: 500/518, Loss: 1.1533\n",
      "Epoch: 02, Step: 100/518, Loss: 1.1252\n",
      "Epoch: 02, Step: 200/518, Loss: 1.1169\n",
      "Epoch: 02, Step: 300/518, Loss: 1.1068\n",
      "Epoch: 02, Step: 400/518, Loss: 1.1041\n",
      "Epoch: 02, Step: 500/518, Loss: 1.1045\n",
      "Epoch: 03, Step: 100/518, Loss: 1.1021\n",
      "Epoch: 03, Step: 200/518, Loss: 1.1017\n",
      "Epoch: 03, Step: 300/518, Loss: 1.1011\n",
      "Epoch: 03, Step: 400/518, Loss: 1.1022\n",
      "Epoch: 03, Step: 500/518, Loss: 1.0981\n",
      "Epoch: 04, Step: 100/518, Loss: 1.1010\n",
      "Epoch: 04, Step: 200/518, Loss: 1.0977\n",
      "Epoch: 04, Step: 300/518, Loss: 1.0987\n",
      "Epoch: 04, Step: 400/518, Loss: 1.0982\n",
      "Epoch: 04, Step: 500/518, Loss: 1.0977\n",
      "Epoch: 05, Step: 100/518, Loss: 1.0987\n",
      "Epoch: 05, Step: 200/518, Loss: 1.0984\n",
      "Epoch: 05, Step: 300/518, Loss: 1.1009\n",
      "Epoch: 05, Step: 400/518, Loss: 1.0988\n",
      "Epoch: 05, Step: 500/518, Loss: 1.0997\n",
      "Epoch: 06, Step: 100/518, Loss: 1.0986\n",
      "Epoch: 06, Step: 200/518, Loss: 1.0972\n",
      "Epoch: 06, Step: 300/518, Loss: 1.0968\n",
      "Epoch: 06, Step: 400/518, Loss: 1.0968\n",
      "Epoch: 06, Step: 500/518, Loss: 1.0958\n",
      "Epoch: 07, Step: 100/518, Loss: 1.1004\n",
      "Epoch: 07, Step: 200/518, Loss: 1.0992\n",
      "Epoch: 07, Step: 300/518, Loss: 1.0982\n",
      "Epoch: 07, Step: 400/518, Loss: 1.0972\n",
      "Epoch: 07, Step: 500/518, Loss: 1.0966\n",
      "Epoch: 08, Step: 100/518, Loss: 1.0996\n",
      "Epoch: 08, Step: 200/518, Loss: 1.0994\n",
      "Epoch: 08, Step: 300/518, Loss: 1.0984\n",
      "Epoch: 08, Step: 400/518, Loss: 1.0989\n",
      "Epoch: 08, Step: 500/518, Loss: 1.0941\n",
      "Epoch: 09, Step: 100/518, Loss: 1.0969\n",
      "Epoch: 09, Step: 200/518, Loss: 1.0935\n",
      "Epoch: 09, Step: 300/518, Loss: 1.0967\n",
      "Epoch: 09, Step: 400/518, Loss: 1.0946\n",
      "Epoch: 09, Step: 500/518, Loss: 1.0968\n",
      "Epoch: 10, Step: 100/518, Loss: 1.0958\n",
      "Epoch: 10, Step: 200/518, Loss: 1.0932\n",
      "Epoch: 10, Step: 300/518, Loss: 1.0969\n",
      "Epoch: 10, Step: 400/518, Loss: 1.0955\n",
      "Epoch: 10, Step: 500/518, Loss: 1.0970\n",
      "Epoch: 11, Step: 100/518, Loss: 1.0975\n",
      "Epoch: 11, Step: 200/518, Loss: 1.0931\n",
      "Epoch: 11, Step: 300/518, Loss: 1.0958\n",
      "Epoch: 11, Step: 400/518, Loss: 1.0923\n",
      "Epoch: 11, Step: 500/518, Loss: 1.0970\n",
      "Epoch: 12, Step: 100/518, Loss: 1.0911\n",
      "Epoch: 12, Step: 200/518, Loss: 1.0963\n",
      "Epoch: 12, Step: 300/518, Loss: 1.0953\n",
      "Epoch: 12, Step: 400/518, Loss: 1.0975\n",
      "Epoch: 12, Step: 500/518, Loss: 1.0945\n",
      "Epoch: 13, Step: 100/518, Loss: 1.0952\n",
      "Epoch: 13, Step: 200/518, Loss: 1.0949\n",
      "Epoch: 13, Step: 300/518, Loss: 1.0960\n",
      "Epoch: 13, Step: 400/518, Loss: 1.0931\n",
      "Epoch: 13, Step: 500/518, Loss: 1.0961\n",
      "Epoch: 14, Step: 100/518, Loss: 1.0951\n",
      "Epoch: 14, Step: 200/518, Loss: 1.0963\n",
      "Epoch: 14, Step: 300/518, Loss: 1.0945\n",
      "Epoch: 14, Step: 400/518, Loss: 1.0957\n",
      "Epoch: 14, Step: 500/518, Loss: 1.0920\n",
      "Epoch: 15, Step: 100/518, Loss: 1.0972\n",
      "Epoch: 15, Step: 200/518, Loss: 1.0931\n",
      "Epoch: 15, Step: 300/518, Loss: 1.0944\n",
      "Epoch: 15, Step: 400/518, Loss: 1.0979\n",
      "Epoch: 15, Step: 500/518, Loss: 1.0918\n",
      "Epoch: 16, Step: 100/518, Loss: 1.0941\n",
      "Epoch: 16, Step: 200/518, Loss: 1.0959\n",
      "Epoch: 16, Step: 300/518, Loss: 1.0942\n",
      "Epoch: 16, Step: 400/518, Loss: 1.0936\n",
      "Epoch: 16, Step: 500/518, Loss: 1.0939\n",
      "Epoch: 17, Step: 100/518, Loss: 1.0938\n",
      "Epoch: 17, Step: 200/518, Loss: 1.0928\n",
      "Epoch: 17, Step: 300/518, Loss: 1.0959\n",
      "Epoch: 17, Step: 400/518, Loss: 1.0934\n",
      "Epoch: 17, Step: 500/518, Loss: 1.0944\n",
      "Epoch: 18, Step: 100/518, Loss: 1.0945\n",
      "Epoch: 18, Step: 200/518, Loss: 1.0958\n",
      "Epoch: 18, Step: 300/518, Loss: 1.0923\n",
      "Epoch: 18, Step: 400/518, Loss: 1.0957\n",
      "Epoch: 18, Step: 500/518, Loss: 1.0953\n",
      "Epoch: 19, Step: 100/518, Loss: 1.0940\n",
      "Epoch: 19, Step: 200/518, Loss: 1.0955\n",
      "Epoch: 19, Step: 300/518, Loss: 1.0957\n",
      "Epoch: 19, Step: 400/518, Loss: 1.0948\n",
      "Epoch: 19, Step: 500/518, Loss: 1.0967\n",
      "Epoch: 20, Step: 100/518, Loss: 1.0956\n",
      "Epoch: 20, Step: 200/518, Loss: 1.0941\n",
      "Epoch: 20, Step: 300/518, Loss: 1.0979\n",
      "Epoch: 20, Step: 400/518, Loss: 1.0930\n",
      "Epoch: 20, Step: 500/518, Loss: 1.0975\n",
      "Epoch: 21, Step: 100/518, Loss: 1.0937\n",
      "Epoch: 21, Step: 200/518, Loss: 1.0964\n",
      "Epoch: 21, Step: 300/518, Loss: 1.0982\n",
      "Epoch: 21, Step: 400/518, Loss: 1.0937\n",
      "Epoch: 21, Step: 500/518, Loss: 1.0939\n",
      "Epoch: 22, Step: 100/518, Loss: 1.0928\n",
      "Epoch: 22, Step: 200/518, Loss: 1.0909\n",
      "Epoch: 22, Step: 300/518, Loss: 1.0938\n",
      "Epoch: 22, Step: 400/518, Loss: 1.0911\n",
      "Epoch: 22, Step: 500/518, Loss: 1.0942\n",
      "Epoch: 23, Step: 100/518, Loss: 1.0926\n",
      "Epoch: 23, Step: 200/518, Loss: 1.0966\n",
      "Epoch: 23, Step: 300/518, Loss: 1.0954\n",
      "Epoch: 23, Step: 400/518, Loss: 1.0926\n",
      "Epoch: 23, Step: 500/518, Loss: 1.0945\n",
      "Epoch: 24, Step: 100/518, Loss: 1.0959\n",
      "Epoch: 24, Step: 200/518, Loss: 1.0943\n",
      "Epoch: 24, Step: 300/518, Loss: 1.0909\n",
      "Epoch: 24, Step: 400/518, Loss: 1.0938\n",
      "Epoch: 24, Step: 500/518, Loss: 1.0945\n",
      "Epoch: 25, Step: 100/518, Loss: 1.0926\n",
      "Epoch: 25, Step: 200/518, Loss: 1.0921\n",
      "Epoch: 25, Step: 300/518, Loss: 1.0942\n",
      "Epoch: 25, Step: 400/518, Loss: 1.0954\n",
      "Epoch: 25, Step: 500/518, Loss: 1.0972\n",
      "Epoch: 26, Step: 100/518, Loss: 1.0913\n",
      "Epoch: 26, Step: 200/518, Loss: 1.0968\n",
      "Epoch: 26, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 26, Step: 400/518, Loss: 1.0922\n",
      "Epoch: 26, Step: 500/518, Loss: 1.0963\n",
      "Epoch: 27, Step: 100/518, Loss: 1.0949\n",
      "Epoch: 27, Step: 200/518, Loss: 1.0944\n",
      "Epoch: 27, Step: 300/518, Loss: 1.0918\n",
      "Epoch: 27, Step: 400/518, Loss: 1.0970\n",
      "Epoch: 27, Step: 500/518, Loss: 1.0950\n",
      "Epoch: 28, Step: 100/518, Loss: 1.0971\n",
      "Epoch: 28, Step: 200/518, Loss: 1.0923\n",
      "Epoch: 28, Step: 300/518, Loss: 1.0944\n",
      "Epoch: 28, Step: 400/518, Loss: 1.0929\n",
      "Epoch: 28, Step: 500/518, Loss: 1.0908\n",
      "Epoch: 29, Step: 100/518, Loss: 1.0934\n",
      "Epoch: 29, Step: 200/518, Loss: 1.0926\n",
      "Epoch: 29, Step: 300/518, Loss: 1.0912\n",
      "Epoch: 29, Step: 400/518, Loss: 1.0938\n",
      "Epoch: 29, Step: 500/518, Loss: 1.0958\n",
      "Epoch: 30, Step: 100/518, Loss: 1.0955\n",
      "Epoch: 30, Step: 200/518, Loss: 1.0954\n",
      "Epoch: 30, Step: 300/518, Loss: 1.0981\n",
      "Epoch: 30, Step: 400/518, Loss: 1.0933\n",
      "Epoch: 30, Step: 500/518, Loss: 1.0982\n",
      "Epoch: 31, Step: 100/518, Loss: 1.0964\n",
      "Epoch: 31, Step: 200/518, Loss: 1.0968\n",
      "Epoch: 31, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 31, Step: 400/518, Loss: 1.0973\n",
      "Epoch: 31, Step: 500/518, Loss: 1.0939\n",
      "Epoch: 32, Step: 100/518, Loss: 1.0933\n",
      "Epoch: 32, Step: 200/518, Loss: 1.0935\n",
      "Epoch: 32, Step: 300/518, Loss: 1.0925\n",
      "Epoch: 32, Step: 400/518, Loss: 1.0951\n",
      "Epoch: 32, Step: 500/518, Loss: 1.0910\n",
      "Epoch: 33, Step: 100/518, Loss: 1.0947\n",
      "Epoch: 33, Step: 200/518, Loss: 1.0943\n",
      "Epoch: 33, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 33, Step: 400/518, Loss: 1.0971\n",
      "Epoch: 33, Step: 500/518, Loss: 1.0941\n",
      "Epoch: 34, Step: 100/518, Loss: 1.0928\n",
      "Epoch: 34, Step: 200/518, Loss: 1.0930\n",
      "Epoch: 34, Step: 300/518, Loss: 1.0954\n",
      "Epoch: 34, Step: 400/518, Loss: 1.0927\n",
      "Epoch: 34, Step: 500/518, Loss: 1.0945\n",
      "Epoch: 35, Step: 100/518, Loss: 1.0939\n",
      "Epoch: 35, Step: 200/518, Loss: 1.0972\n",
      "Epoch: 35, Step: 300/518, Loss: 1.0944\n",
      "Epoch: 35, Step: 400/518, Loss: 1.0949\n",
      "Epoch: 35, Step: 500/518, Loss: 1.0931\n",
      "Epoch: 36, Step: 100/518, Loss: 1.0926\n",
      "Epoch: 36, Step: 200/518, Loss: 1.0949\n",
      "Epoch: 36, Step: 300/518, Loss: 1.0939\n",
      "Epoch: 36, Step: 400/518, Loss: 1.0939\n",
      "Epoch: 36, Step: 500/518, Loss: 1.0952\n",
      "Epoch: 37, Step: 100/518, Loss: 1.0945\n",
      "Epoch: 37, Step: 200/518, Loss: 1.0923\n",
      "Epoch: 37, Step: 300/518, Loss: 1.0961\n",
      "Epoch: 37, Step: 400/518, Loss: 1.0914\n",
      "Epoch: 37, Step: 500/518, Loss: 1.0931\n",
      "Epoch: 38, Step: 100/518, Loss: 1.0943\n",
      "Epoch: 38, Step: 200/518, Loss: 1.0941\n",
      "Epoch: 38, Step: 300/518, Loss: 1.0936\n",
      "Epoch: 38, Step: 400/518, Loss: 1.0930\n",
      "Epoch: 38, Step: 500/518, Loss: 1.0938\n",
      "Epoch: 39, Step: 100/518, Loss: 1.0969\n",
      "Epoch: 39, Step: 200/518, Loss: 1.0950\n",
      "Epoch: 39, Step: 300/518, Loss: 1.0950\n",
      "Epoch: 39, Step: 400/518, Loss: 1.0971\n",
      "Epoch: 39, Step: 500/518, Loss: 1.0962\n",
      "Epoch: 40, Step: 100/518, Loss: 1.0936\n",
      "Epoch: 40, Step: 200/518, Loss: 1.0935\n",
      "Epoch: 40, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 40, Step: 400/518, Loss: 1.0961\n",
      "Epoch: 40, Step: 500/518, Loss: 1.0980\n",
      "Epoch: 41, Step: 100/518, Loss: 1.0946\n",
      "Epoch: 41, Step: 200/518, Loss: 1.0908\n",
      "Epoch: 41, Step: 300/518, Loss: 1.0926\n",
      "Epoch: 41, Step: 400/518, Loss: 1.0993\n",
      "Epoch: 41, Step: 500/518, Loss: 1.0973\n",
      "Epoch: 42, Step: 100/518, Loss: 1.0927\n",
      "Epoch: 42, Step: 200/518, Loss: 1.0960\n",
      "Epoch: 42, Step: 300/518, Loss: 1.0936\n",
      "Epoch: 42, Step: 400/518, Loss: 1.0936\n",
      "Epoch: 42, Step: 500/518, Loss: 1.0925\n",
      "Epoch: 43, Step: 100/518, Loss: 1.0938\n",
      "Epoch: 43, Step: 200/518, Loss: 1.0883\n",
      "Epoch: 43, Step: 300/518, Loss: 1.0927\n",
      "Epoch: 43, Step: 400/518, Loss: 1.0959\n",
      "Epoch: 43, Step: 500/518, Loss: 1.0968\n",
      "Epoch: 44, Step: 100/518, Loss: 1.0910\n",
      "Epoch: 44, Step: 200/518, Loss: 1.0977\n",
      "Epoch: 44, Step: 300/518, Loss: 1.0946\n",
      "Epoch: 44, Step: 400/518, Loss: 1.0939\n",
      "Epoch: 44, Step: 500/518, Loss: 1.0958\n",
      "Epoch: 45, Step: 100/518, Loss: 1.0937\n",
      "Epoch: 45, Step: 200/518, Loss: 1.0949\n",
      "Epoch: 45, Step: 300/518, Loss: 1.0933\n",
      "Epoch: 45, Step: 400/518, Loss: 1.0940\n",
      "Epoch: 45, Step: 500/518, Loss: 1.0920\n",
      "Epoch: 46, Step: 100/518, Loss: 1.0928\n",
      "Epoch: 46, Step: 200/518, Loss: 1.0932\n",
      "Epoch: 46, Step: 300/518, Loss: 1.0916\n",
      "Epoch: 46, Step: 400/518, Loss: 1.0950\n",
      "Epoch: 46, Step: 500/518, Loss: 1.0924\n",
      "Epoch: 47, Step: 100/518, Loss: 1.0942\n",
      "Epoch: 47, Step: 200/518, Loss: 1.0918\n",
      "Epoch: 47, Step: 300/518, Loss: 1.0946\n",
      "Epoch: 47, Step: 400/518, Loss: 1.0957\n",
      "Epoch: 47, Step: 500/518, Loss: 1.0957\n",
      "Epoch: 48, Step: 100/518, Loss: 1.0940\n",
      "Epoch: 48, Step: 200/518, Loss: 1.0944\n",
      "Epoch: 48, Step: 300/518, Loss: 1.0935\n",
      "Epoch: 48, Step: 400/518, Loss: 1.0953\n",
      "Epoch: 48, Step: 500/518, Loss: 1.0896\n",
      "Epoch: 49, Step: 100/518, Loss: 1.0951\n",
      "Epoch: 49, Step: 200/518, Loss: 1.0923\n",
      "Epoch: 49, Step: 300/518, Loss: 1.0925\n",
      "Epoch: 49, Step: 400/518, Loss: 1.0921\n",
      "Epoch: 49, Step: 500/518, Loss: 1.0923\n",
      "Epoch: 50, Step: 100/518, Loss: 1.0921\n",
      "Epoch: 50, Step: 200/518, Loss: 1.0915\n",
      "Epoch: 50, Step: 300/518, Loss: 1.0957\n",
      "Epoch: 50, Step: 400/518, Loss: 1.0947\n",
      "Epoch: 50, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 51, Step: 100/518, Loss: 1.0928\n",
      "Epoch: 51, Step: 200/518, Loss: 1.0934\n",
      "Epoch: 51, Step: 300/518, Loss: 1.0923\n",
      "Epoch: 51, Step: 400/518, Loss: 1.0921\n",
      "Epoch: 51, Step: 500/518, Loss: 1.0927\n",
      "Epoch: 52, Step: 100/518, Loss: 1.0970\n",
      "Epoch: 52, Step: 200/518, Loss: 1.0982\n",
      "Epoch: 52, Step: 300/518, Loss: 1.0978\n",
      "Epoch: 52, Step: 400/518, Loss: 1.0956\n",
      "Epoch: 52, Step: 500/518, Loss: 1.0927\n",
      "Epoch: 53, Step: 100/518, Loss: 1.0967\n",
      "Epoch: 53, Step: 200/518, Loss: 1.0931\n",
      "Epoch: 53, Step: 300/518, Loss: 1.0961\n",
      "Epoch: 53, Step: 400/518, Loss: 1.0981\n",
      "Epoch: 53, Step: 500/518, Loss: 1.0960\n",
      "Epoch: 54, Step: 100/518, Loss: 1.0914\n",
      "Epoch: 54, Step: 200/518, Loss: 1.0952\n",
      "Epoch: 54, Step: 300/518, Loss: 1.0944\n",
      "Epoch: 54, Step: 400/518, Loss: 1.0933\n",
      "Epoch: 54, Step: 500/518, Loss: 1.0939\n",
      "Epoch: 55, Step: 100/518, Loss: 1.0927\n",
      "Epoch: 55, Step: 200/518, Loss: 1.0936\n",
      "Epoch: 55, Step: 300/518, Loss: 1.0937\n",
      "Epoch: 55, Step: 400/518, Loss: 1.0926\n",
      "Epoch: 55, Step: 500/518, Loss: 1.0917\n",
      "Epoch: 56, Step: 100/518, Loss: 1.0933\n",
      "Epoch: 56, Step: 200/518, Loss: 1.0928\n",
      "Epoch: 56, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 56, Step: 400/518, Loss: 1.0981\n",
      "Epoch: 56, Step: 500/518, Loss: 1.0953\n",
      "Epoch: 57, Step: 100/518, Loss: 1.0946\n",
      "Epoch: 57, Step: 200/518, Loss: 1.0971\n",
      "Epoch: 57, Step: 300/518, Loss: 1.0940\n",
      "Epoch: 57, Step: 400/518, Loss: 1.0934\n",
      "Epoch: 57, Step: 500/518, Loss: 1.0935\n",
      "Epoch: 58, Step: 100/518, Loss: 1.0962\n",
      "Epoch: 58, Step: 200/518, Loss: 1.0905\n",
      "Epoch: 58, Step: 300/518, Loss: 1.0918\n",
      "Epoch: 58, Step: 400/518, Loss: 1.0941\n",
      "Epoch: 58, Step: 500/518, Loss: 1.0946\n",
      "Epoch: 59, Step: 100/518, Loss: 1.0915\n",
      "Epoch: 59, Step: 200/518, Loss: 1.0938\n",
      "Epoch: 59, Step: 300/518, Loss: 1.0921\n",
      "Epoch: 59, Step: 400/518, Loss: 1.0921\n",
      "Epoch: 59, Step: 500/518, Loss: 1.0948\n",
      "Epoch: 60, Step: 100/518, Loss: 1.0967\n",
      "Epoch: 60, Step: 200/518, Loss: 1.0960\n",
      "Epoch: 60, Step: 300/518, Loss: 1.0917\n",
      "Epoch: 60, Step: 400/518, Loss: 1.0927\n",
      "Epoch: 60, Step: 500/518, Loss: 1.0910\n",
      "Epoch: 61, Step: 100/518, Loss: 1.0928\n",
      "Epoch: 61, Step: 200/518, Loss: 1.0967\n",
      "Epoch: 61, Step: 300/518, Loss: 1.0964\n",
      "Epoch: 61, Step: 400/518, Loss: 1.0935\n",
      "Epoch: 61, Step: 500/518, Loss: 1.0885\n",
      "Epoch: 62, Step: 100/518, Loss: 1.0950\n",
      "Epoch: 62, Step: 200/518, Loss: 1.0934\n",
      "Epoch: 62, Step: 300/518, Loss: 1.0940\n",
      "Epoch: 62, Step: 400/518, Loss: 1.0940\n",
      "Epoch: 62, Step: 500/518, Loss: 1.0997\n",
      "Epoch: 63, Step: 100/518, Loss: 1.0964\n",
      "Epoch: 63, Step: 200/518, Loss: 1.0939\n",
      "Epoch: 63, Step: 300/518, Loss: 1.0928\n",
      "Epoch: 63, Step: 400/518, Loss: 1.0955\n",
      "Epoch: 63, Step: 500/518, Loss: 1.0932\n",
      "Epoch: 64, Step: 100/518, Loss: 1.0946\n",
      "Epoch: 64, Step: 200/518, Loss: 1.0941\n",
      "Epoch: 64, Step: 300/518, Loss: 1.0937\n",
      "Epoch: 64, Step: 400/518, Loss: 1.0940\n",
      "Epoch: 64, Step: 500/518, Loss: 1.0915\n",
      "Epoch: 65, Step: 100/518, Loss: 1.0942\n",
      "Epoch: 65, Step: 200/518, Loss: 1.0937\n",
      "Epoch: 65, Step: 300/518, Loss: 1.0909\n",
      "Epoch: 65, Step: 400/518, Loss: 1.0953\n",
      "Epoch: 65, Step: 500/518, Loss: 1.0930\n",
      "Epoch: 66, Step: 100/518, Loss: 1.0908\n",
      "Epoch: 66, Step: 200/518, Loss: 1.0939\n",
      "Epoch: 66, Step: 300/518, Loss: 1.0968\n",
      "Epoch: 66, Step: 400/518, Loss: 1.0950\n",
      "Epoch: 66, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 67, Step: 100/518, Loss: 1.0930\n",
      "Epoch: 67, Step: 200/518, Loss: 1.0925\n",
      "Epoch: 67, Step: 300/518, Loss: 1.0916\n",
      "Epoch: 67, Step: 400/518, Loss: 1.0951\n",
      "Epoch: 67, Step: 500/518, Loss: 1.0932\n",
      "Epoch: 68, Step: 100/518, Loss: 1.0950\n",
      "Epoch: 68, Step: 200/518, Loss: 1.0964\n",
      "Epoch: 68, Step: 300/518, Loss: 1.0930\n",
      "Epoch: 68, Step: 400/518, Loss: 1.0964\n",
      "Epoch: 68, Step: 500/518, Loss: 1.0936\n",
      "Epoch: 69, Step: 100/518, Loss: 1.0926\n",
      "Epoch: 69, Step: 200/518, Loss: 1.0956\n",
      "Epoch: 69, Step: 300/518, Loss: 1.0928\n",
      "Epoch: 69, Step: 400/518, Loss: 1.0951\n",
      "Epoch: 69, Step: 500/518, Loss: 1.0934\n",
      "Epoch: 70, Step: 100/518, Loss: 1.0981\n",
      "Epoch: 70, Step: 200/518, Loss: 1.0943\n",
      "Epoch: 70, Step: 300/518, Loss: 1.0956\n",
      "Epoch: 70, Step: 400/518, Loss: 1.0939\n",
      "Epoch: 70, Step: 500/518, Loss: 1.0953\n",
      "Epoch: 71, Step: 100/518, Loss: 1.0938\n",
      "Epoch: 71, Step: 200/518, Loss: 1.0937\n",
      "Epoch: 71, Step: 300/518, Loss: 1.0946\n",
      "Epoch: 71, Step: 400/518, Loss: 1.0931\n",
      "Epoch: 71, Step: 500/518, Loss: 1.0933\n",
      "Epoch: 72, Step: 100/518, Loss: 1.0977\n",
      "Epoch: 72, Step: 200/518, Loss: 1.0898\n",
      "Epoch: 72, Step: 300/518, Loss: 1.0935\n",
      "Epoch: 72, Step: 400/518, Loss: 1.0944\n",
      "Epoch: 72, Step: 500/518, Loss: 1.0942\n",
      "Epoch: 73, Step: 100/518, Loss: 1.0951\n",
      "Epoch: 73, Step: 200/518, Loss: 1.0903\n",
      "Epoch: 73, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 73, Step: 400/518, Loss: 1.0948\n",
      "Epoch: 73, Step: 500/518, Loss: 1.0964\n",
      "Epoch: 74, Step: 100/518, Loss: 1.0936\n",
      "Epoch: 74, Step: 200/518, Loss: 1.0935\n",
      "Epoch: 74, Step: 300/518, Loss: 1.0934\n",
      "Epoch: 74, Step: 400/518, Loss: 1.0982\n",
      "Epoch: 74, Step: 500/518, Loss: 1.0949\n",
      "Epoch: 75, Step: 100/518, Loss: 1.0929\n",
      "Epoch: 75, Step: 200/518, Loss: 1.0929\n",
      "Epoch: 75, Step: 300/518, Loss: 1.0949\n",
      "Epoch: 75, Step: 400/518, Loss: 1.0948\n",
      "Epoch: 75, Step: 500/518, Loss: 1.0929\n",
      "Epoch: 76, Step: 100/518, Loss: 1.0915\n",
      "Epoch: 76, Step: 200/518, Loss: 1.0973\n",
      "Epoch: 76, Step: 300/518, Loss: 1.0932\n",
      "Epoch: 76, Step: 400/518, Loss: 1.0910\n",
      "Epoch: 76, Step: 500/518, Loss: 1.0944\n",
      "Epoch: 77, Step: 100/518, Loss: 1.0905\n",
      "Epoch: 77, Step: 200/518, Loss: 1.0957\n",
      "Epoch: 77, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 77, Step: 400/518, Loss: 1.0948\n",
      "Epoch: 77, Step: 500/518, Loss: 1.0982\n",
      "Epoch: 78, Step: 100/518, Loss: 1.0938\n",
      "Epoch: 78, Step: 200/518, Loss: 1.0916\n",
      "Epoch: 78, Step: 300/518, Loss: 1.0948\n",
      "Epoch: 78, Step: 400/518, Loss: 1.0932\n",
      "Epoch: 78, Step: 500/518, Loss: 1.0926\n",
      "Epoch: 79, Step: 100/518, Loss: 1.0915\n",
      "Epoch: 79, Step: 200/518, Loss: 1.0959\n",
      "Epoch: 79, Step: 300/518, Loss: 1.0966\n",
      "Epoch: 79, Step: 400/518, Loss: 1.0942\n",
      "Epoch: 79, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 80, Step: 100/518, Loss: 1.0934\n",
      "Epoch: 80, Step: 200/518, Loss: 1.0917\n",
      "Epoch: 80, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 80, Step: 400/518, Loss: 1.0922\n",
      "Epoch: 80, Step: 500/518, Loss: 1.0941\n",
      "Epoch: 81, Step: 100/518, Loss: 1.0986\n",
      "Epoch: 81, Step: 200/518, Loss: 1.0904\n",
      "Epoch: 81, Step: 300/518, Loss: 1.0921\n",
      "Epoch: 81, Step: 400/518, Loss: 1.0947\n",
      "Epoch: 81, Step: 500/518, Loss: 1.0948\n",
      "Epoch: 82, Step: 100/518, Loss: 1.0953\n",
      "Epoch: 82, Step: 200/518, Loss: 1.0935\n",
      "Epoch: 82, Step: 300/518, Loss: 1.0889\n",
      "Epoch: 82, Step: 400/518, Loss: 1.0959\n",
      "Epoch: 82, Step: 500/518, Loss: 1.0934\n",
      "Epoch: 83, Step: 100/518, Loss: 1.0911\n",
      "Epoch: 83, Step: 200/518, Loss: 1.0913\n",
      "Epoch: 83, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 83, Step: 400/518, Loss: 1.0917\n",
      "Epoch: 83, Step: 500/518, Loss: 1.0904\n",
      "Epoch: 84, Step: 100/518, Loss: 1.0966\n",
      "Epoch: 84, Step: 200/518, Loss: 1.0930\n",
      "Epoch: 84, Step: 300/518, Loss: 1.0937\n",
      "Epoch: 84, Step: 400/518, Loss: 1.0948\n",
      "Epoch: 84, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 85, Step: 100/518, Loss: 1.0934\n",
      "Epoch: 85, Step: 200/518, Loss: 1.0972\n",
      "Epoch: 85, Step: 300/518, Loss: 1.0928\n",
      "Epoch: 85, Step: 400/518, Loss: 1.0946\n",
      "Epoch: 85, Step: 500/518, Loss: 1.0938\n",
      "Epoch: 86, Step: 100/518, Loss: 1.0922\n",
      "Epoch: 86, Step: 200/518, Loss: 1.0933\n",
      "Epoch: 86, Step: 300/518, Loss: 1.0932\n",
      "Epoch: 86, Step: 400/518, Loss: 1.0936\n",
      "Epoch: 86, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 87, Step: 100/518, Loss: 1.0938\n",
      "Epoch: 87, Step: 200/518, Loss: 1.0971\n",
      "Epoch: 87, Step: 300/518, Loss: 1.0958\n",
      "Epoch: 87, Step: 400/518, Loss: 1.0947\n",
      "Epoch: 87, Step: 500/518, Loss: 1.0958\n",
      "Epoch: 88, Step: 100/518, Loss: 1.0956\n",
      "Epoch: 88, Step: 200/518, Loss: 1.0950\n",
      "Epoch: 88, Step: 300/518, Loss: 1.0934\n",
      "Epoch: 88, Step: 400/518, Loss: 1.0925\n",
      "Epoch: 88, Step: 500/518, Loss: 1.0968\n",
      "Epoch: 89, Step: 100/518, Loss: 1.0929\n",
      "Epoch: 89, Step: 200/518, Loss: 1.0960\n",
      "Epoch: 89, Step: 300/518, Loss: 1.0936\n",
      "Epoch: 89, Step: 400/518, Loss: 1.0922\n",
      "Epoch: 89, Step: 500/518, Loss: 1.0955\n",
      "Epoch: 90, Step: 100/518, Loss: 1.0953\n",
      "Epoch: 90, Step: 200/518, Loss: 1.0939\n",
      "Epoch: 90, Step: 300/518, Loss: 1.0964\n",
      "Epoch: 90, Step: 400/518, Loss: 1.0918\n",
      "Epoch: 90, Step: 500/518, Loss: 1.0964\n",
      "Epoch: 91, Step: 100/518, Loss: 1.0957\n",
      "Epoch: 91, Step: 200/518, Loss: 1.0937\n",
      "Epoch: 91, Step: 300/518, Loss: 1.0955\n",
      "Epoch: 91, Step: 400/518, Loss: 1.0918\n",
      "Epoch: 91, Step: 500/518, Loss: 1.0945\n",
      "Epoch: 92, Step: 100/518, Loss: 1.0940\n",
      "Epoch: 92, Step: 200/518, Loss: 1.0934\n",
      "Epoch: 92, Step: 300/518, Loss: 1.0917\n",
      "Epoch: 92, Step: 400/518, Loss: 1.0926\n",
      "Epoch: 92, Step: 500/518, Loss: 1.0951\n",
      "Epoch: 93, Step: 100/518, Loss: 1.0917\n",
      "Epoch: 93, Step: 200/518, Loss: 1.0974\n",
      "Epoch: 93, Step: 300/518, Loss: 1.0931\n",
      "Epoch: 93, Step: 400/518, Loss: 1.0936\n",
      "Epoch: 93, Step: 500/518, Loss: 1.0913\n",
      "Epoch: 94, Step: 100/518, Loss: 1.0940\n",
      "Epoch: 94, Step: 200/518, Loss: 1.0929\n",
      "Epoch: 94, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 94, Step: 400/518, Loss: 1.0950\n",
      "Epoch: 94, Step: 500/518, Loss: 1.0926\n",
      "Epoch: 95, Step: 100/518, Loss: 1.0947\n",
      "Epoch: 95, Step: 200/518, Loss: 1.0968\n",
      "Epoch: 95, Step: 300/518, Loss: 1.0899\n",
      "Epoch: 95, Step: 400/518, Loss: 1.0930\n",
      "Epoch: 95, Step: 500/518, Loss: 1.0916\n",
      "Epoch: 96, Step: 100/518, Loss: 1.0954\n",
      "Epoch: 96, Step: 200/518, Loss: 1.0936\n",
      "Epoch: 96, Step: 300/518, Loss: 1.0947\n",
      "Epoch: 96, Step: 400/518, Loss: 1.0904\n",
      "Epoch: 96, Step: 500/518, Loss: 1.0944\n",
      "Epoch: 97, Step: 100/518, Loss: 1.0949\n",
      "Epoch: 97, Step: 200/518, Loss: 1.0946\n",
      "Epoch: 97, Step: 300/518, Loss: 1.0962\n",
      "Epoch: 97, Step: 400/518, Loss: 1.0935\n",
      "Epoch: 97, Step: 500/518, Loss: 1.0972\n",
      "Epoch: 98, Step: 100/518, Loss: 1.0964\n",
      "Epoch: 98, Step: 200/518, Loss: 1.0932\n",
      "Epoch: 98, Step: 300/518, Loss: 1.0952\n",
      "Epoch: 98, Step: 400/518, Loss: 1.0920\n",
      "Epoch: 98, Step: 500/518, Loss: 1.0931\n",
      "Epoch: 99, Step: 100/518, Loss: 1.0940\n",
      "Epoch: 99, Step: 200/518, Loss: 1.0948\n",
      "Epoch: 99, Step: 300/518, Loss: 1.0930\n",
      "Epoch: 99, Step: 400/518, Loss: 1.0924\n",
      "Epoch: 99, Step: 500/518, Loss: 1.0974\n",
      "Epoch: 100, Step: 100/518, Loss: 1.0989\n",
      "Epoch: 100, Step: 200/518, Loss: 1.0949\n",
      "Epoch: 100, Step: 300/518, Loss: 1.0951\n",
      "Epoch: 100, Step: 400/518, Loss: 1.0936\n",
      "Epoch: 100, Step: 500/518, Loss: 1.0927\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, '\n",
    "                  f'Loss: {loss:.4f}')\n",
    "\n",
    "        if (i + 1) % 10 == 0:  # Save model every 10 steps.\n",
    "            save_embedding(model)\n",
    "    save_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 hours\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)[train_idx]\n",
    "    loss = criterion(out, y_true[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = f'{args.device}' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins')\n",
    "split_idx = dataset.get_idx_split()\n",
    "data = dataset[0]\n",
    "\n",
    "x = scatter(data.edge_attr, data.edge_index[0], dim=0,\n",
    "            dim_size=data.num_nodes, reduce='mean').to('cpu')\n",
    "\n",
    "if args.use_node_embedding:\n",
    "    embedding = torch.load('embedding.pt', map_location='cpu')\n",
    "    x = torch.cat([x, embedding], dim=-1)\n",
    "\n",
    "x = x.to(device)\n",
    "y_true = data.y.to(device)\n",
    "train_idx = split_idx['train'].to(device)\n",
    "\n",
    "model = MLP(x.size(-1), args.hidden_channels, 112, args.num_layers,\n",
    "            args.dropout).to(device)\n",
    "\n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 01, Loss: 0.6916, Train: 42.99%, Valid: 44.82% Test: 44.55%\n",
      "Run: 01, Epoch: 02, Loss: 0.6201, Train: 42.78%, Valid: 45.31% Test: 45.95%\n",
      "Run: 01, Epoch: 03, Loss: 0.4820, Train: 46.30%, Valid: 48.87% Test: 49.84%\n",
      "Run: 01, Epoch: 04, Loss: 0.3747, Train: 58.75%, Valid: 57.45% Test: 58.17%\n",
      "Run: 01, Epoch: 05, Loss: 0.4017, Train: 67.66%, Valid: 64.69% Test: 62.91%\n",
      "Run: 01, Epoch: 06, Loss: 0.3927, Train: 71.53%, Valid: 68.02% Test: 64.31%\n",
      "Run: 01, Epoch: 07, Loss: 0.3519, Train: 73.32%, Valid: 69.36% Test: 64.76%\n",
      "Run: 01, Epoch: 08, Loss: 0.3271, Train: 74.19%, Valid: 69.94% Test: 64.86%\n",
      "Run: 01, Epoch: 09, Loss: 0.3267, Train: 74.68%, Valid: 70.29% Test: 64.93%\n",
      "Run: 01, Epoch: 10, Loss: 0.3347, Train: 74.99%, Valid: 70.54% Test: 65.00%\n",
      "Run: 01, Epoch: 11, Loss: 0.3379, Train: 75.19%, Valid: 70.76% Test: 65.09%\n",
      "Run: 01, Epoch: 12, Loss: 0.3337, Train: 75.22%, Valid: 70.88% Test: 65.19%\n",
      "Run: 01, Epoch: 13, Loss: 0.3260, Train: 75.10%, Valid: 70.86% Test: 65.28%\n",
      "Run: 01, Epoch: 14, Loss: 0.3199, Train: 75.12%, Valid: 71.00% Test: 65.42%\n",
      "Run: 01, Epoch: 15, Loss: 0.3184, Train: 75.24%, Valid: 71.24% Test: 65.59%\n",
      "Run: 01, Epoch: 16, Loss: 0.3208, Train: 75.49%, Valid: 71.58% Test: 65.77%\n",
      "Run: 01, Epoch: 17, Loss: 0.3237, Train: 75.66%, Valid: 71.81% Test: 65.89%\n",
      "Run: 01, Epoch: 18, Loss: 0.3241, Train: 75.76%, Valid: 71.94% Test: 65.97%\n",
      "Run: 01, Epoch: 19, Loss: 0.3209, Train: 75.88%, Valid: 72.05% Test: 66.04%\n",
      "Run: 01, Epoch: 20, Loss: 0.3162, Train: 76.00%, Valid: 72.14% Test: 66.11%\n",
      "Run: 01, Epoch: 21, Loss: 0.3123, Train: 76.11%, Valid: 72.22% Test: 66.20%\n",
      "Run: 01, Epoch: 22, Loss: 0.3104, Train: 76.25%, Valid: 72.32% Test: 66.32%\n",
      "Run: 01, Epoch: 23, Loss: 0.3102, Train: 76.39%, Valid: 72.43% Test: 66.48%\n",
      "Run: 01, Epoch: 24, Loss: 0.3104, Train: 76.51%, Valid: 72.53% Test: 66.67%\n",
      "Run: 01, Epoch: 25, Loss: 0.3098, Train: 76.62%, Valid: 72.62% Test: 66.86%\n",
      "Run: 01, Epoch: 26, Loss: 0.3084, Train: 76.70%, Valid: 72.70% Test: 67.06%\n",
      "Run: 01, Epoch: 27, Loss: 0.3066, Train: 76.76%, Valid: 72.77% Test: 67.24%\n",
      "Run: 01, Epoch: 28, Loss: 0.3054, Train: 76.82%, Valid: 72.81% Test: 67.41%\n",
      "Run: 01, Epoch: 29, Loss: 0.3050, Train: 76.88%, Valid: 72.84% Test: 67.55%\n",
      "Run: 01, Epoch: 30, Loss: 0.3053, Train: 76.95%, Valid: 72.87% Test: 67.68%\n",
      "Run: 01, Epoch: 31, Loss: 0.3053, Train: 77.04%, Valid: 72.89% Test: 67.79%\n",
      "Run: 01, Epoch: 32, Loss: 0.3044, Train: 77.14%, Valid: 72.92% Test: 67.88%\n",
      "Run: 01, Epoch: 33, Loss: 0.3032, Train: 77.27%, Valid: 72.94% Test: 67.97%\n",
      "Run: 01, Epoch: 34, Loss: 0.3020, Train: 77.41%, Valid: 72.95% Test: 68.05%\n",
      "Run: 01, Epoch: 35, Loss: 0.3013, Train: 77.56%, Valid: 72.96% Test: 68.14%\n",
      "Run: 01, Epoch: 36, Loss: 0.3009, Train: 77.70%, Valid: 72.97% Test: 68.22%\n",
      "Run: 01, Epoch: 37, Loss: 0.3004, Train: 77.84%, Valid: 72.99% Test: 68.31%\n",
      "Run: 01, Epoch: 38, Loss: 0.2997, Train: 77.98%, Valid: 73.03% Test: 68.40%\n",
      "Run: 01, Epoch: 39, Loss: 0.2986, Train: 78.11%, Valid: 73.09% Test: 68.50%\n",
      "Run: 01, Epoch: 40, Loss: 0.2975, Train: 78.25%, Valid: 73.16% Test: 68.59%\n",
      "Run: 01, Epoch: 41, Loss: 0.2966, Train: 78.39%, Valid: 73.25% Test: 68.70%\n",
      "Run: 01, Epoch: 42, Loss: 0.2960, Train: 78.55%, Valid: 73.35% Test: 68.79%\n",
      "Run: 01, Epoch: 43, Loss: 0.2953, Train: 78.73%, Valid: 73.47% Test: 68.88%\n",
      "Run: 01, Epoch: 44, Loss: 0.2943, Train: 78.94%, Valid: 73.60% Test: 68.97%\n",
      "Run: 01, Epoch: 45, Loss: 0.2931, Train: 79.16%, Valid: 73.74% Test: 69.06%\n",
      "Run: 01, Epoch: 46, Loss: 0.2918, Train: 79.39%, Valid: 73.87% Test: 69.15%\n",
      "Run: 01, Epoch: 47, Loss: 0.2907, Train: 79.62%, Valid: 74.00% Test: 69.23%\n",
      "Run: 01, Epoch: 48, Loss: 0.2897, Train: 79.84%, Valid: 74.14% Test: 69.33%\n",
      "Run: 01, Epoch: 49, Loss: 0.2886, Train: 80.04%, Valid: 74.29% Test: 69.44%\n",
      "Run: 01, Epoch: 50, Loss: 0.2874, Train: 80.23%, Valid: 74.44% Test: 69.56%\n",
      "Run: 01, Epoch: 51, Loss: 0.2862, Train: 80.41%, Valid: 74.58% Test: 69.69%\n",
      "Run: 01, Epoch: 52, Loss: 0.2849, Train: 80.57%, Valid: 74.72% Test: 69.83%\n",
      "Run: 01, Epoch: 53, Loss: 0.2839, Train: 80.74%, Valid: 74.86% Test: 69.97%\n",
      "Run: 01, Epoch: 54, Loss: 0.2830, Train: 80.92%, Valid: 75.00% Test: 70.12%\n",
      "Run: 01, Epoch: 55, Loss: 0.2821, Train: 81.10%, Valid: 75.16% Test: 70.30%\n",
      "Run: 01, Epoch: 56, Loss: 0.2810, Train: 81.30%, Valid: 75.35% Test: 70.51%\n",
      "Run: 01, Epoch: 57, Loss: 0.2800, Train: 81.51%, Valid: 75.57% Test: 70.76%\n",
      "Run: 01, Epoch: 58, Loss: 0.2791, Train: 81.73%, Valid: 75.82% Test: 71.03%\n",
      "Run: 01, Epoch: 59, Loss: 0.2780, Train: 81.96%, Valid: 76.09% Test: 71.33%\n",
      "Run: 01, Epoch: 60, Loss: 0.2767, Train: 82.18%, Valid: 76.35% Test: 71.63%\n",
      "Run: 01, Epoch: 61, Loss: 0.2755, Train: 82.38%, Valid: 76.59% Test: 71.94%\n",
      "Run: 01, Epoch: 62, Loss: 0.2741, Train: 82.59%, Valid: 76.81% Test: 72.23%\n",
      "Run: 01, Epoch: 63, Loss: 0.2730, Train: 82.78%, Valid: 77.02% Test: 72.51%\n",
      "Run: 01, Epoch: 64, Loss: 0.2718, Train: 82.97%, Valid: 77.20% Test: 72.75%\n",
      "Run: 01, Epoch: 65, Loss: 0.2707, Train: 83.14%, Valid: 77.37% Test: 72.98%\n",
      "Run: 01, Epoch: 66, Loss: 0.2696, Train: 83.30%, Valid: 77.52% Test: 73.17%\n",
      "Run: 01, Epoch: 67, Loss: 0.2687, Train: 83.44%, Valid: 77.65% Test: 73.35%\n",
      "Run: 01, Epoch: 68, Loss: 0.2676, Train: 83.56%, Valid: 77.76% Test: 73.52%\n",
      "Run: 01, Epoch: 69, Loss: 0.2668, Train: 83.67%, Valid: 77.86% Test: 73.67%\n",
      "Run: 01, Epoch: 70, Loss: 0.2660, Train: 83.79%, Valid: 77.94% Test: 73.81%\n",
      "Run: 01, Epoch: 71, Loss: 0.2652, Train: 83.89%, Valid: 78.00% Test: 73.92%\n",
      "Run: 01, Epoch: 72, Loss: 0.2645, Train: 84.01%, Valid: 78.05% Test: 74.03%\n",
      "Run: 01, Epoch: 73, Loss: 0.2637, Train: 84.12%, Valid: 78.10% Test: 74.12%\n",
      "Run: 01, Epoch: 74, Loss: 0.2629, Train: 84.23%, Valid: 78.15% Test: 74.21%\n",
      "Run: 01, Epoch: 75, Loss: 0.2622, Train: 84.35%, Valid: 78.20% Test: 74.31%\n",
      "Run: 01, Epoch: 76, Loss: 0.2615, Train: 84.47%, Valid: 78.25% Test: 74.41%\n",
      "Run: 01, Epoch: 77, Loss: 0.2607, Train: 84.59%, Valid: 78.30% Test: 74.51%\n",
      "Run: 01, Epoch: 78, Loss: 0.2598, Train: 84.70%, Valid: 78.33% Test: 74.61%\n",
      "Run: 01, Epoch: 79, Loss: 0.2590, Train: 84.82%, Valid: 78.36% Test: 74.69%\n",
      "Run: 01, Epoch: 80, Loss: 0.2584, Train: 84.93%, Valid: 78.37% Test: 74.76%\n",
      "Run: 01, Epoch: 81, Loss: 0.2576, Train: 85.04%, Valid: 78.38% Test: 74.84%\n",
      "Run: 01, Epoch: 82, Loss: 0.2569, Train: 85.15%, Valid: 78.39% Test: 74.92%\n",
      "Run: 01, Epoch: 83, Loss: 0.2561, Train: 85.25%, Valid: 78.39% Test: 74.99%\n",
      "Run: 01, Epoch: 84, Loss: 0.2555, Train: 85.36%, Valid: 78.38% Test: 75.05%\n",
      "Run: 01, Epoch: 85, Loss: 0.2549, Train: 85.46%, Valid: 78.36% Test: 75.08%\n",
      "Run: 01, Epoch: 86, Loss: 0.2542, Train: 85.56%, Valid: 78.34% Test: 75.08%\n",
      "Run: 01, Epoch: 87, Loss: 0.2534, Train: 85.66%, Valid: 78.32% Test: 75.07%\n",
      "Run: 01, Epoch: 88, Loss: 0.2528, Train: 85.76%, Valid: 78.31% Test: 75.05%\n",
      "Run: 01, Epoch: 89, Loss: 0.2522, Train: 85.85%, Valid: 78.30% Test: 75.02%\n",
      "Run: 01, Epoch: 90, Loss: 0.2514, Train: 85.94%, Valid: 78.29% Test: 74.98%\n",
      "Run: 01, Epoch: 91, Loss: 0.2508, Train: 86.03%, Valid: 78.28% Test: 74.94%\n",
      "Run: 01, Epoch: 92, Loss: 0.2502, Train: 86.12%, Valid: 78.28% Test: 74.92%\n",
      "Run: 01, Epoch: 93, Loss: 0.2495, Train: 86.21%, Valid: 78.29% Test: 74.91%\n",
      "Run: 01, Epoch: 94, Loss: 0.2490, Train: 86.30%, Valid: 78.30% Test: 74.92%\n",
      "Run: 01, Epoch: 95, Loss: 0.2485, Train: 86.38%, Valid: 78.30% Test: 74.93%\n",
      "Run: 01, Epoch: 96, Loss: 0.2477, Train: 86.47%, Valid: 78.29% Test: 74.95%\n",
      "Run: 01, Epoch: 97, Loss: 0.2471, Train: 86.55%, Valid: 78.29% Test: 74.98%\n",
      "Run: 01, Epoch: 98, Loss: 0.2465, Train: 86.62%, Valid: 78.28% Test: 75.01%\n",
      "Run: 01, Epoch: 99, Loss: 0.2461, Train: 86.70%, Valid: 78.28% Test: 75.04%\n",
      "Run: 01, Epoch: 100, Loss: 0.2455, Train: 86.77%, Valid: 78.27% Test: 75.03%\n",
      "Run 01:\n",
      "Highest Train: 86.77\n",
      "Highest Valid: 78.39\n",
      "  Final Train: 85.15\n",
      "   Final Test: 74.92\n",
      "All runs:\n",
      "Highest Train: 86.77 ± nan\n",
      "Highest Valid: 78.39 ± nan\n",
      "  Final Train: 85.15 ± nan\n",
      "   Final Test: 74.92 ± nan\n"
     ]
    }
   ],
   "source": [
    "for run in range(args.runs):\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    for epoch in range(1, 1 + args.epochs):\n",
    "        loss = train(model, x, y_true, train_idx, optimizer)\n",
    "\n",
    "        if epoch % args.eval_steps == 0:\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_rocauc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_rocauc:.2f}% '\n",
    "                      f'Test: {100 * test_rocauc:.2f}%')\n",
    "\n",
    "    logger.print_statistics(run)\n",
    "logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
