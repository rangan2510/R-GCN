{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Install prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import humanize\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ogb\r\n",
      "  Downloading ogb-1.2.0-py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 982 kB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (0.23.1)\r\n",
      "Requirement already satisfied: pandas>=0.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.0.3)\r\n",
      "Requirement already satisfied: torch>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.5.0)\r\n",
      "Requirement already satisfied: urllib3>=1.24.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.24.3)\r\n",
      "Requirement already satisfied: tqdm>=4.29.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (4.45.0)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from ogb) (1.18.1)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (2.1.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24.0->ogb) (2.8.1)\r\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch>=1.2.0->ogb) (0.18.2)\r\n",
      "Installing collected packages: ogb\r\n",
      "Successfully installed ogb-1.2.0\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-scatter==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 12.3 MB 470 kB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-scatter\r\n",
      "Successfully installed torch-scatter-2.0.4\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-sparse==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (21.6 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 21.6 MB 5.0 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-sparse==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-sparse==latest+cu101) (1.18.1)\r\n",
      "Installing collected packages: torch-sparse\r\n",
      "Successfully installed torch-sparse-0.6.5\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-cluster==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (18.2 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 18.2 MB 4.3 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-cluster==latest+cu101) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scipy->torch-cluster==latest+cu101) (1.18.1)\r\n",
      "Installing collected packages: torch-cluster\r\n",
      "Successfully installed torch-cluster-1.5.4\r\n",
      "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\r\n",
      "Collecting torch-spline-conv==latest+cu101\r\n",
      "  Downloading https://pytorch-geometric.com/whl/torch-1.5.0/torch_spline_conv-latest%2Bcu101-cp37-cp37m-linux_x86_64.whl (6.3 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 6.3 MB 3.0 MB/s \r\n",
      "\u001b[?25hInstalling collected packages: torch-spline-conv\r\n",
      "Successfully installed torch-spline-conv-1.2.0\r\n",
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-1.5.0.tar.gz (153 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 153 kB 3.5 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.5.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.18.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (4.45.0)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.4.1)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.4)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.23.1)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (0.48.0)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.23.0)\r\n",
      "Collecting plyfile\r\n",
      "  Downloading plyfile-0.7.2-py3-none-any.whl (39 kB)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (1.0.3)\r\n",
      "Collecting rdflib\r\n",
      "  Downloading rdflib-5.0.0-py3-none-any.whl (231 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 231 kB 8.4 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from torch-geometric) (2.10.0)\r\n",
      "Collecting googledrivedownloader\r\n",
      "  Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB)\r\n",
      "Collecting ase\r\n",
      "  Downloading ase-3.19.1-py3-none-any.whl (2.1 MB)\r\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 11.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torch-geometric) (0.18.2)\r\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->torch-geometric) (4.4.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (0.14.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->torch-geometric) (2.1.0)\r\n",
      "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (0.31.0)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from numba->torch-geometric) (46.1.3.post20200325)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2.9)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (1.24.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (2020.4.5.1)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torch-geometric) (3.0.4)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.7/site-packages (from pandas->torch-geometric) (2.8.1)\r\n",
      "Collecting isodate\r\n",
      "  Downloading isodate-0.6.0-py2.py3-none-any.whl (45 kB)\r\n",
      "\u001b[K     |████████████████████████████████| 45 kB 2.6 MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: pyparsing in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (2.4.7)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from rdflib->torch-geometric) (1.14.0)\r\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from ase->torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->ase->torch-geometric) (0.10.0)\r\n",
      "Building wheels for collected packages: torch-geometric\r\n",
      "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for torch-geometric: filename=torch_geometric-1.5.0-py3-none-any.whl size=267915 sha256=f50bea8cf4f4e0b8bc6ee91166c31c5f48639315a0cdf0b966dfe4089e44d0c5\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/87/09/84/1d026c4e02fc66af6224faaa7c35db83a178ec6593e69baf06\r\n",
      "Successfully built torch-geometric\r\n",
      "Installing collected packages: plyfile, isodate, rdflib, googledrivedownloader, ase, torch-geometric\r\n",
      "Successfully installed ase-3.19.1 googledrivedownloader-0.4 isodate-0.6.0 plyfile-0.7.2 rdflib-5.0.0 torch-geometric-1.5.0\r\n"
     ]
    }
   ],
   "source": [
    "# install Open Graph Benchmark\n",
    "! pip install ogb\n",
    "\n",
    "# install PyTorch Geometric\n",
    "!pip install torch-scatter==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-sparse==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-cluster==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-spline-conv==latest+cu101 -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch_sparse import SparseTensor\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.nn.inits import glorot, zeros\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    def __init__(self, runs, info=None):\n",
    "        self.info = info\n",
    "        self.results = [[] for _ in range(runs)]\n",
    "\n",
    "    def add_result(self, run, result):\n",
    "        assert len(result) == 3\n",
    "        assert run >= 0 and run < len(self.results)\n",
    "        self.results[run].append(result)\n",
    "\n",
    "    def print_statistics(self, run=None):\n",
    "        if run is not None:\n",
    "            result = 100 * torch.tensor(self.results[run])\n",
    "            argmax = result[:, 1].argmax().item()\n",
    "            print(f'Run {run + 1:02d}:')\n",
    "            print(f'Highest Train: {result[:, 0].max():.2f}')\n",
    "            print(f'Highest Valid: {result[:, 1].max():.2f}')\n",
    "            print(f'  Final Train: {result[argmax, 0]:.2f}')\n",
    "            print(f'   Final Test: {result[argmax, 2]:.2f}')\n",
    "        else:\n",
    "            result = 100 * torch.tensor(self.results)\n",
    "\n",
    "            best_results = []\n",
    "            for r in result:\n",
    "                train1 = r[:, 0].max().item()\n",
    "                valid = r[:, 1].max().item()\n",
    "                train2 = r[r[:, 1].argmax(), 0].item()\n",
    "                test = r[r[:, 1].argmax(), 2].item()\n",
    "                best_results.append((train1, valid, train2, test))\n",
    "\n",
    "            best_result = torch.tensor(best_results)\n",
    "\n",
    "            print(f'All runs:')\n",
    "            r = best_result[:, 0]\n",
    "            print(f'Highest Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 1]\n",
    "            print(f'Highest Valid: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 2]\n",
    "            print(f'  Final Train: {r.mean():.2f} ± {r.std():.2f}')\n",
    "            r = best_result[:, 3]\n",
    "            print(f'   Final Test: {r.mean():.2f} ± {r.std():.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    log_steps = 1\n",
    "    num_layers = 3\n",
    "    skip_layers = 3\n",
    "    hidden_channels = 256\n",
    "    dropout = 0.05\n",
    "    lr = 0.01\n",
    "    epochs = 1000\n",
    "    eval_steps = 10\n",
    "    runs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/216 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://snap.stanford.edu/ogb/data/nodeproppred/proteinfunc.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded 0.21 GB: 100%|██████████| 216/216 [00:06<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting dataset/proteinfunc.zip\n",
      "Processing...\n",
      "Loading necessary files...\n",
      "This might take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing graphs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.22s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 442.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting graphs into PyG objects...\n",
      "Saving...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(args.device)\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-proteins')\n",
    "split_idx = dataset.get_idx_split()\n",
    "data = dataset[0]\n",
    "\n",
    "x = scatter(data.edge_attr, data.edge_index[0], dim=0,\n",
    "            dim_size=data.num_nodes, reduce='mean').to(device)\n",
    "\n",
    "y_true = data.y.to(device)\n",
    "train_idx = split_idx['train'].to(device)\n",
    "\n",
    "edge_index = data.edge_index.to(device)\n",
    "adj = SparseTensor(row=edge_index[0], col=edge_index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        zeros(self.bias)\n",
    "        \n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        return adj @ x @ self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGEConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(SAGEConv, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.root_weight = Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight)\n",
    "        glorot(self.root_weight)\n",
    "        zeros(self.bias)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        out = adj.matmul(x, reduce=\"mean\") @ self.weight\n",
    "        out = out + x @ self.root_weight + self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualGC(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, skip_n_channels,dropout):\n",
    "        super(ResidualGC, self).__init__()\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(skip_n_channels):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        \n",
    "        self.compress = SAGEConv(hidden_channels + in_channels, hidden_channels)\n",
    "        \n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "        \n",
    "        self.dropout = dropout\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "        self.compress.reset_parameters()\n",
    "            \n",
    "    def forward(self, x, adj):\n",
    "        x_i = x #identity\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = torch.cat([x_i,x],1)\n",
    "        \n",
    "        x = self.compress(x, adj)\n",
    "        x = self.convs[-1](x, adj)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(RGCN, self).__init__()\n",
    "        self.init_conv = GCNConv(in_channels, hidden_channels)\n",
    "        self.res1 = ResidualGC(hidden_channels,hidden_channels, hidden_channels,3, dropout)    \n",
    "        self.bridge = GCNConv(hidden_channels, hidden_channels)     \n",
    "        self.res2 = ResidualGC(hidden_channels,hidden_channels, out_channels,3, dropout)\n",
    "        self.final_conv = GCNConv(out_channels, out_channels)    \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.init_conv.reset_parameters()\n",
    "        self.res1.reset_parameters()\n",
    "        self.bridge.reset_parameters()\n",
    "        self.res2.reset_parameters()\n",
    "        self.final_conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.init_conv(x,adj)\n",
    "        x = self.res1(x,adj)\n",
    "        x = self.bridge(x,adj)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.res2(x,adj)\n",
    "        x = self.final_conv(x,adj)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RRGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(RRGCN, self).__init__()\n",
    "        self.init_conv = GCNConv(in_channels, hidden_channels)\n",
    "        self.res1 = ResidualGC(hidden_channels,hidden_channels, hidden_channels,3, dropout)    \n",
    "        self.bridge = SAGEConv(hidden_channels, hidden_channels)     \n",
    "        self.res2 = ResidualGC(hidden_channels,hidden_channels, hidden_channels,3, dropout)\n",
    "        self.compress = SAGEConv(hidden_channels*2, hidden_channels)\n",
    "        self.final_conv = GCNConv(hidden_channels, out_channels)    \n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.init_conv.reset_parameters()\n",
    "        self.res1.reset_parameters()\n",
    "        self.bridge.reset_parameters()\n",
    "        self.res2.reset_parameters()\n",
    "        self.final_conv.reset_parameters()\n",
    "        self.compress.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.init_conv(x,adj)\n",
    "        x_i = x\n",
    "        x = self.res1(x,adj)\n",
    "        x = self.bridge(x,adj)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.res2(x,adj)\n",
    "        x = torch.cat([x,x_i],1) \n",
    "        x = self.compress(x,adj)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.final_conv(x,adj)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(SAGE, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, adj)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, adj)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers,\n",
    "                 dropout):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.lins = torch.nn.ModuleList()\n",
    "        self.lins.append(torch.nn.Linear(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.lins.append(torch.nn.Linear(hidden_channels, hidden_channels))\n",
    "        self.lins.append(torch.nn.Linear(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for lin in self.lins:\n",
    "            lin.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for lin in self.lins[:-1]:\n",
    "            x = lin(x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.lins[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, adj, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, adj)[train_idx]\n",
    "    loss = criterion(out, y_true[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, x, adj, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(x, adj)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "models.append(GCN(x.size(-1), args.hidden_channels, 112, args.num_layers,args.dropout).to(device))\n",
    "\n",
    "models.append(SAGE(x.size(-1), args.hidden_channels, 112, args.num_layers,args.dropout).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01\t Epoch: 10\t Loss: 0.4876\t Train: 35.85%\t Valid: 34.56%\tTest: 38.73%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 20\t Loss: 0.4325\t Train: 41.20%\t Valid: 33.54%\tTest: 37.36%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.4127\t Train: 46.44%\t Valid: 34.44%\tTest: 37.01%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.4091\t Train: 48.73%\t Valid: 35.40%\tTest: 37.48%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.4038\t Train: 49.72%\t Valid: 36.79%\tTest: 39.11%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3998\t Train: 50.48%\t Valid: 37.43%\tTest: 39.41%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3955\t Train: 51.02%\t Valid: 38.70%\tTest: 40.47%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3911\t Train: 52.18%\t Valid: 41.12%\tTest: 42.03%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3867\t Train: 53.35%\t Valid: 43.85%\tTest: 43.78%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3821\t Train: 54.92%\t Valid: 47.06%\tTest: 45.81%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3774\t Train: 56.55%\t Valid: 49.58%\tTest: 47.42%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3733\t Train: 58.40%\t Valid: 52.05%\tTest: 48.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3698\t Train: 59.81%\t Valid: 53.97%\tTest: 50.06%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3668\t Train: 60.94%\t Valid: 55.37%\tTest: 50.94%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.3644\t Train: 62.20%\t Valid: 56.89%\tTest: 51.96%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.3624\t Train: 62.94%\t Valid: 57.88%\tTest: 52.69%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 170\t Loss: 0.3607\t Train: 63.47%\t Valid: 58.57%\tTest: 53.32%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.3591\t Train: 63.99%\t Valid: 59.30%\tTest: 54.19%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.3577\t Train: 64.73%\t Valid: 60.25%\tTest: 55.30%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 200\t Loss: 0.3565\t Train: 65.48%\t Valid: 61.23%\tTest: 56.34%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 210\t Loss: 0.3553\t Train: 65.75%\t Valid: 61.64%\tTest: 56.87%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 220\t Loss: 0.3545\t Train: 66.00%\t Valid: 62.01%\tTest: 57.29%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.3536\t Train: 66.40%\t Valid: 62.53%\tTest: 58.02%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 240\t Loss: 0.3525\t Train: 66.14%\t Valid: 62.27%\tTest: 57.95%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.3517\t Train: 66.73%\t Valid: 63.05%\tTest: 58.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 260\t Loss: 0.3512\t Train: 66.87%\t Valid: 63.25%\tTest: 58.93%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.3504\t Train: 66.75%\t Valid: 63.15%\tTest: 58.99%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 280\t Loss: 0.3492\t Train: 67.96%\t Valid: 64.60%\tTest: 60.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.3482\t Train: 67.93%\t Valid: 64.52%\tTest: 60.23%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.3472\t Train: 68.01%\t Valid: 64.65%\tTest: 60.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 310\t Loss: 0.3472\t Train: 69.15%\t Valid: 65.91%\tTest: 61.70%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 320\t Loss: 0.3457\t Train: 68.51%\t Valid: 65.24%\tTest: 61.36%\n",
      "Run: 01\t Epoch: 330\t Loss: 0.3451\t Train: 68.94%\t Valid: 65.73%\tTest: 61.89%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 340\t Loss: 0.3442\t Train: 69.28%\t Valid: 66.07%\tTest: 62.14%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 350\t Loss: 0.3436\t Train: 69.20%\t Valid: 65.94%\tTest: 62.05%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.3464\t Train: 70.14%\t Valid: 66.97%\tTest: 62.65%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 370\t Loss: 0.3434\t Train: 68.94%\t Valid: 65.56%\tTest: 61.68%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.3426\t Train: 70.14%\t Valid: 66.98%\tTest: 62.52%\n",
      "Run: 01\t Epoch: 390\t Loss: 0.3415\t Train: 69.43%\t Valid: 66.13%\tTest: 62.22%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.3410\t Train: 69.67%\t Valid: 66.39%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 410\t Loss: 0.3403\t Train: 70.03%\t Valid: 66.82%\tTest: 62.75%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.3399\t Train: 70.01%\t Valid: 66.75%\tTest: 62.73%\n",
      "Run: 01\t Epoch: 430\t Loss: 0.3398\t Train: 71.59%\t Valid: 68.16%\tTest: 62.94%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 440\t Loss: 0.3411\t Train: 69.57%\t Valid: 66.21%\tTest: 62.47%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.3392\t Train: 70.28%\t Valid: 66.93%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 460\t Loss: 0.3384\t Train: 70.58%\t Valid: 67.35%\tTest: 62.86%\n",
      "Run: 01\t Epoch: 470\t Loss: 0.3380\t Train: 70.74%\t Valid: 67.50%\tTest: 62.98%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 480\t Loss: 0.3374\t Train: 71.06%\t Valid: 67.81%\tTest: 63.23%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 490\t Loss: 0.3390\t Train: 70.09%\t Valid: 66.58%\tTest: 62.34%\n",
      "Run: 01\t Epoch: 500\t Loss: 0.3375\t Train: 70.40%\t Valid: 67.06%\tTest: 62.72%\n",
      "Run: 01\t Epoch: 510\t Loss: 0.3371\t Train: 70.15%\t Valid: 66.74%\tTest: 62.45%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.3360\t Train: 71.40%\t Valid: 68.06%\tTest: 63.14%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.3367\t Train: 70.68%\t Valid: 67.15%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 540\t Loss: 0.3363\t Train: 70.35%\t Valid: 66.90%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 550\t Loss: 0.3353\t Train: 70.53%\t Valid: 66.99%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.3341\t Train: 71.46%\t Valid: 68.01%\tTest: 62.86%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.3345\t Train: 72.54%\t Valid: 68.90%\tTest: 62.96%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.3338\t Train: 72.08%\t Valid: 68.58%\tTest: 63.01%\n",
      "Run: 01\t Epoch: 590\t Loss: 0.3331\t Train: 72.15%\t Valid: 68.56%\tTest: 62.99%\n",
      "Run: 01\t Epoch: 600\t Loss: 0.3333\t Train: 71.16%\t Valid: 67.54%\tTest: 62.48%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.3331\t Train: 71.39%\t Valid: 67.70%\tTest: 62.23%\n",
      "Run: 01\t Epoch: 620\t Loss: 0.3346\t Train: 72.37%\t Valid: 68.76%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.3325\t Train: 71.68%\t Valid: 68.10%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 640\t Loss: 0.3329\t Train: 71.50%\t Valid: 67.97%\tTest: 62.28%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.3309\t Train: 72.99%\t Valid: 69.43%\tTest: 63.07%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.3311\t Train: 73.18%\t Valid: 69.57%\tTest: 62.96%\n",
      "Run: 01\t Epoch: 670\t Loss: 0.3304\t Train: 73.00%\t Valid: 69.35%\tTest: 62.81%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.3301\t Train: 72.55%\t Valid: 68.85%\tTest: 62.56%\n",
      "Run: 01\t Epoch: 690\t Loss: 0.3295\t Train: 72.05%\t Valid: 68.48%\tTest: 62.40%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.3293\t Train: 72.26%\t Valid: 68.65%\tTest: 62.21%\n",
      "Run: 01\t Epoch: 710\t Loss: 0.3368\t Train: 70.39%\t Valid: 66.66%\tTest: 60.66%\n",
      "Run: 01\t Epoch: 720\t Loss: 0.3294\t Train: 73.24%\t Valid: 69.59%\tTest: 62.43%\n",
      "Run: 01\t Epoch: 730\t Loss: 0.3289\t Train: 72.71%\t Valid: 69.07%\tTest: 62.44%\n",
      "Run: 01\t Epoch: 740\t Loss: 0.3287\t Train: 72.01%\t Valid: 68.37%\tTest: 62.07%\n",
      "Run: 01\t Epoch: 750\t Loss: 0.3280\t Train: 72.32%\t Valid: 68.59%\tTest: 61.95%\n",
      "Run: 01\t Epoch: 760\t Loss: 0.3279\t Train: 72.34%\t Valid: 68.64%\tTest: 62.03%\n",
      "Run: 01\t Epoch: 770\t Loss: 0.3274\t Train: 73.47%\t Valid: 69.74%\tTest: 62.39%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.3300\t Train: 72.82%\t Valid: 69.04%\tTest: 61.25%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.3277\t Train: 73.67%\t Valid: 70.11%\tTest: 62.63%\n",
      "Run: 01\t Epoch: 800\t Loss: 0.3272\t Train: 73.32%\t Valid: 69.57%\tTest: 62.18%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.3272\t Train: 73.46%\t Valid: 69.69%\tTest: 62.01%\n",
      "Run: 01\t Epoch: 820\t Loss: 0.3281\t Train: 72.18%\t Valid: 68.48%\tTest: 61.50%\n",
      "Run: 01\t Epoch: 830\t Loss: 0.3336\t Train: 75.00%\t Valid: 71.22%\tTest: 62.33%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.3285\t Train: 73.64%\t Valid: 70.03%\tTest: 62.08%\n",
      "Run: 01\t Epoch: 850\t Loss: 0.3257\t Train: 72.83%\t Valid: 69.09%\tTest: 61.72%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.3255\t Train: 72.99%\t Valid: 69.29%\tTest: 61.68%\n",
      "Run: 01\t Epoch: 870\t Loss: 0.3252\t Train: 74.13%\t Valid: 70.40%\tTest: 62.02%\n",
      "Run: 01\t Epoch: 880\t Loss: 0.3265\t Train: 72.41%\t Valid: 68.62%\tTest: 60.58%\n",
      "Run: 01\t Epoch: 890\t Loss: 0.3253\t Train: 72.48%\t Valid: 68.85%\tTest: 61.47%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.3245\t Train: 73.49%\t Valid: 69.80%\tTest: 61.62%\n",
      "Run: 01\t Epoch: 910\t Loss: 0.3242\t Train: 73.78%\t Valid: 69.98%\tTest: 61.62%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.3249\t Train: 74.07%\t Valid: 70.28%\tTest: 61.51%\n",
      "Run: 01\t Epoch: 930\t Loss: 0.3244\t Train: 74.33%\t Valid: 70.12%\tTest: 60.76%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.3261\t Train: 74.67%\t Valid: 71.00%\tTest: 61.75%\n",
      "Run: 01\t Epoch: 950\t Loss: 0.3235\t Train: 73.56%\t Valid: 69.78%\tTest: 60.87%\n",
      "Run: 01\t Epoch: 960\t Loss: 0.3248\t Train: 72.87%\t Valid: 69.16%\tTest: 60.77%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.3237\t Train: 73.40%\t Valid: 69.61%\tTest: 60.75%\n",
      "Run: 01\t Epoch: 980\t Loss: 0.3234\t Train: 72.52%\t Valid: 68.69%\tTest: 60.46%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.3258\t Train: 72.62%\t Valid: 68.78%\tTest: 60.09%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.3226\t Train: 74.67%\t Valid: 70.75%\tTest: 61.07%\n",
      "Run 01:\n",
      "Highest Train: 75.00\n",
      "Highest Valid: 71.22\n",
      "  Final Train: 75.00\n",
      "   Final Test: 62.33\n",
      "All runs:\n",
      "Highest Train: 75.00 ± nan\n",
      "Highest Valid: 71.22 ± nan\n",
      "  Final Train: 75.00 ± nan\n",
      "   Final Test: 62.33 ± nan\n",
      "================================================================================\n",
      "Run: 01\t Epoch: 10\t Loss: 0.3506\t Train: 61.68%\t Valid: 60.78%\tTest: 54.22%\n",
      "Run: 01\t Epoch: 20\t Loss: 0.3473\t Train: 66.56%\t Valid: 61.29%\tTest: 55.44%\n",
      "Run: 01\t Epoch: 30\t Loss: 0.3318\t Train: 67.45%\t Valid: 62.61%\tTest: 57.47%\n",
      "Run: 01\t Epoch: 40\t Loss: 0.3282\t Train: 68.60%\t Valid: 64.68%\tTest: 59.41%\n",
      "Run: 01\t Epoch: 50\t Loss: 0.3246\t Train: 69.91%\t Valid: 66.42%\tTest: 61.97%\n",
      "Run: 01\t Epoch: 60\t Loss: 0.3211\t Train: 71.10%\t Valid: 68.08%\tTest: 64.21%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 70\t Loss: 0.3179\t Train: 71.89%\t Valid: 68.63%\tTest: 65.27%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 80\t Loss: 0.3150\t Train: 72.84%\t Valid: 69.49%\tTest: 65.72%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 90\t Loss: 0.3120\t Train: 73.89%\t Valid: 70.27%\tTest: 66.28%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 100\t Loss: 0.3089\t Train: 74.97%\t Valid: 71.23%\tTest: 67.35%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 110\t Loss: 0.3060\t Train: 75.94%\t Valid: 72.06%\tTest: 67.99%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 120\t Loss: 0.3038\t Train: 76.59%\t Valid: 72.64%\tTest: 68.43%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 130\t Loss: 0.3020\t Train: 77.10%\t Valid: 73.15%\tTest: 68.73%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 140\t Loss: 0.3004\t Train: 77.53%\t Valid: 73.53%\tTest: 69.13%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 150\t Loss: 0.2992\t Train: 77.91%\t Valid: 73.90%\tTest: 69.44%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 160\t Loss: 0.2978\t Train: 78.28%\t Valid: 74.23%\tTest: 69.64%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 170\t Loss: 0.2964\t Train: 78.58%\t Valid: 74.50%\tTest: 69.95%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 180\t Loss: 0.2954\t Train: 78.80%\t Valid: 74.67%\tTest: 70.11%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 190\t Loss: 0.2947\t Train: 79.04%\t Valid: 74.85%\tTest: 70.25%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 200\t Loss: 0.2937\t Train: 79.24%\t Valid: 75.03%\tTest: 70.39%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 210\t Loss: 0.2929\t Train: 79.39%\t Valid: 75.23%\tTest: 70.56%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 220\t Loss: 0.2922\t Train: 79.55%\t Valid: 75.40%\tTest: 70.74%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 230\t Loss: 0.2935\t Train: 79.60%\t Valid: 75.51%\tTest: 70.86%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 240\t Loss: 0.2913\t Train: 79.75%\t Valid: 75.60%\tTest: 70.81%\n",
      "Run: 01\t Epoch: 250\t Loss: 0.2907\t Train: 79.89%\t Valid: 75.73%\tTest: 70.90%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 260\t Loss: 0.2902\t Train: 80.01%\t Valid: 75.88%\tTest: 71.03%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 270\t Loss: 0.2894\t Train: 80.12%\t Valid: 75.99%\tTest: 71.11%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 280\t Loss: 0.2891\t Train: 80.22%\t Valid: 76.12%\tTest: 71.20%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 290\t Loss: 0.2888\t Train: 80.33%\t Valid: 76.21%\tTest: 71.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 300\t Loss: 0.2887\t Train: 80.37%\t Valid: 76.23%\tTest: 71.19%\n",
      "Run: 01\t Epoch: 310\t Loss: 0.2876\t Train: 80.50%\t Valid: 76.39%\tTest: 71.37%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 320\t Loss: 0.2870\t Train: 80.60%\t Valid: 76.47%\tTest: 71.46%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 330\t Loss: 0.2865\t Train: 80.68%\t Valid: 76.55%\tTest: 71.45%\n",
      "Run: 01\t Epoch: 340\t Loss: 0.2862\t Train: 80.76%\t Valid: 76.65%\tTest: 71.55%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 350\t Loss: 0.2855\t Train: 80.84%\t Valid: 76.75%\tTest: 71.54%\n",
      "Run: 01\t Epoch: 360\t Loss: 0.2851\t Train: 80.95%\t Valid: 76.83%\tTest: 71.61%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 370\t Loss: 0.2861\t Train: 80.90%\t Valid: 76.84%\tTest: 71.46%\n",
      "Run: 01\t Epoch: 380\t Loss: 0.2846\t Train: 80.99%\t Valid: 76.83%\tTest: 71.71%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 390\t Loss: 0.2843\t Train: 81.09%\t Valid: 76.92%\tTest: 71.55%\n",
      "Run: 01\t Epoch: 400\t Loss: 0.2836\t Train: 81.23%\t Valid: 77.06%\tTest: 71.79%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 410\t Loss: 0.2831\t Train: 81.33%\t Valid: 77.25%\tTest: 72.00%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 420\t Loss: 0.2826\t Train: 81.41%\t Valid: 77.34%\tTest: 72.06%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 430\t Loss: 0.2821\t Train: 81.48%\t Valid: 77.38%\tTest: 72.03%\n",
      "Run: 01\t Epoch: 440\t Loss: 0.2832\t Train: 81.41%\t Valid: 77.45%\tTest: 71.92%\n",
      "Run: 01\t Epoch: 450\t Loss: 0.2819\t Train: 81.46%\t Valid: 77.31%\tTest: 72.07%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 460\t Loss: 0.2824\t Train: 81.57%\t Valid: 77.53%\tTest: 72.15%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 470\t Loss: 0.2814\t Train: 81.68%\t Valid: 77.55%\tTest: 72.21%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 480\t Loss: 0.2804\t Train: 81.76%\t Valid: 77.74%\tTest: 72.29%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 490\t Loss: 0.2802\t Train: 81.83%\t Valid: 77.84%\tTest: 72.43%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 500\t Loss: 0.2797\t Train: 81.90%\t Valid: 77.90%\tTest: 72.50%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 510\t Loss: 0.2796\t Train: 81.92%\t Valid: 78.01%\tTest: 72.50%\n",
      "Run: 01\t Epoch: 520\t Loss: 0.2815\t Train: 81.82%\t Valid: 77.84%\tTest: 72.22%\n",
      "Run: 01\t Epoch: 530\t Loss: 0.2808\t Train: 81.96%\t Valid: 77.92%\tTest: 72.59%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 540\t Loss: 0.2788\t Train: 82.06%\t Valid: 78.20%\tTest: 72.85%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 550\t Loss: 0.2784\t Train: 82.12%\t Valid: 78.20%\tTest: 72.69%\n",
      "Run: 01\t Epoch: 560\t Loss: 0.2779\t Train: 82.18%\t Valid: 78.21%\tTest: 72.78%\n",
      "Run: 01\t Epoch: 570\t Loss: 0.2786\t Train: 82.17%\t Valid: 78.33%\tTest: 72.75%\n",
      "Run: 01\t Epoch: 580\t Loss: 0.2779\t Train: 82.25%\t Valid: 78.34%\tTest: 72.86%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 590\t Loss: 0.2795\t Train: 82.22%\t Valid: 78.28%\tTest: 72.97%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 600\t Loss: 0.2781\t Train: 82.26%\t Valid: 78.37%\tTest: 72.95%\n",
      "Run: 01\t Epoch: 610\t Loss: 0.2773\t Train: 82.32%\t Valid: 78.47%\tTest: 73.04%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 620\t Loss: 0.2767\t Train: 82.40%\t Valid: 78.56%\tTest: 73.04%\n",
      "Run: 01\t Epoch: 630\t Loss: 0.2770\t Train: 82.43%\t Valid: 78.47%\tTest: 73.10%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 640\t Loss: 0.2768\t Train: 82.45%\t Valid: 78.61%\tTest: 73.08%\n",
      "Run: 01\t Epoch: 650\t Loss: 0.2773\t Train: 82.46%\t Valid: 78.50%\tTest: 73.06%\n",
      "Run: 01\t Epoch: 660\t Loss: 0.2759\t Train: 82.54%\t Valid: 78.60%\tTest: 73.23%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 670\t Loss: 0.2761\t Train: 82.56%\t Valid: 78.72%\tTest: 73.20%\n",
      "Run: 01\t Epoch: 680\t Loss: 0.2765\t Train: 82.61%\t Valid: 78.66%\tTest: 73.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 690\t Loss: 0.2762\t Train: 82.63%\t Valid: 78.80%\tTest: 73.27%\n",
      "Run: 01\t Epoch: 700\t Loss: 0.2750\t Train: 82.70%\t Valid: 78.78%\tTest: 73.35%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 710\t Loss: 0.2750\t Train: 82.72%\t Valid: 78.87%\tTest: 73.39%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 720\t Loss: 0.2757\t Train: 82.74%\t Valid: 78.82%\tTest: 73.48%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 730\t Loss: 0.2741\t Train: 82.80%\t Valid: 78.91%\tTest: 73.50%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 740\t Loss: 0.2738\t Train: 82.83%\t Valid: 78.97%\tTest: 73.56%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 750\t Loss: 0.2737\t Train: 82.84%\t Valid: 78.95%\tTest: 73.65%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 760\t Loss: 0.2746\t Train: 82.85%\t Valid: 79.08%\tTest: 73.81%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 770\t Loss: 0.2738\t Train: 82.90%\t Valid: 79.12%\tTest: 73.76%\n",
      "Run: 01\t Epoch: 780\t Loss: 0.2740\t Train: 82.95%\t Valid: 78.98%\tTest: 73.69%\n",
      "Run: 01\t Epoch: 790\t Loss: 0.2730\t Train: 82.98%\t Valid: 79.17%\tTest: 73.91%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 800\t Loss: 0.2735\t Train: 82.97%\t Valid: 78.99%\tTest: 73.69%\n",
      "Run: 01\t Epoch: 810\t Loss: 0.2727\t Train: 83.04%\t Valid: 79.19%\tTest: 73.99%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 820\t Loss: 0.2730\t Train: 83.03%\t Valid: 79.27%\tTest: 74.06%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 830\t Loss: 0.2733\t Train: 83.09%\t Valid: 79.17%\tTest: 74.03%\n",
      "Run: 01\t Epoch: 840\t Loss: 0.2729\t Train: 83.11%\t Valid: 79.33%\tTest: 74.19%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 850\t Loss: 0.2737\t Train: 83.12%\t Valid: 79.13%\tTest: 74.01%\n",
      "Run: 01\t Epoch: 860\t Loss: 0.2717\t Train: 83.17%\t Valid: 79.23%\tTest: 74.19%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 870\t Loss: 0.2717\t Train: 83.21%\t Valid: 79.36%\tTest: 74.31%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 880\t Loss: 0.2716\t Train: 83.23%\t Valid: 79.40%\tTest: 74.33%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 890\t Loss: 0.2714\t Train: 83.26%\t Valid: 79.30%\tTest: 74.23%\n",
      "Run: 01\t Epoch: 900\t Loss: 0.2708\t Train: 83.29%\t Valid: 79.45%\tTest: 74.41%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 910\t Loss: 0.2711\t Train: 83.28%\t Valid: 79.28%\tTest: 74.16%\n",
      "Run: 01\t Epoch: 920\t Loss: 0.2715\t Train: 83.33%\t Valid: 79.49%\tTest: 74.46%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 930\t Loss: 0.2703\t Train: 83.36%\t Valid: 79.50%\tTest: 74.45%\n",
      "Run: 01\t Epoch: 940\t Loss: 0.2703\t Train: 83.40%\t Valid: 79.53%\tTest: 74.53%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 950\t Loss: 0.2705\t Train: 83.40%\t Valid: 79.56%\tTest: 74.57%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 960\t Loss: 0.2702\t Train: 83.42%\t Valid: 79.40%\tTest: 74.40%\n",
      "Run: 01\t Epoch: 970\t Loss: 0.2702\t Train: 83.45%\t Valid: 79.59%\tTest: 74.66%\n",
      "Model saved.\n",
      "Run: 01\t Epoch: 980\t Loss: 0.2698\t Train: 83.49%\t Valid: 79.50%\tTest: 74.52%\n",
      "Run: 01\t Epoch: 990\t Loss: 0.2698\t Train: 83.50%\t Valid: 79.65%\tTest: 74.64%\n",
      "Run: 01\t Epoch: 1000\t Loss: 0.2707\t Train: 83.51%\t Valid: 79.48%\tTest: 74.49%\n",
      "Run 01:\n",
      "Highest Train: 83.51\n",
      "Highest Valid: 79.65\n",
      "  Final Train: 83.50\n",
      "   Final Test: 74.64\n",
      "All runs:\n",
      "Highest Train: 83.51 ± nan\n",
      "Highest Valid: 79.65 ± nan\n",
      "  Final Train: 83.50 ± nan\n",
      "   Final Test: 74.64 ± nan\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Pre-compute GCN normalization.\n",
    "adj = adj.set_diag()\n",
    "deg = adj.sum(dim=1).to(torch.float)\n",
    "deg_inv_sqrt = deg.pow(-0.5)\n",
    "deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "adj = deg_inv_sqrt.view(-1, 1) * adj * deg_inv_sqrt.view(1, -1)\n",
    "    \n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "best_test_score = 0\n",
    "\n",
    "for model in models:\n",
    "    for run in range(args.runs):\n",
    "        model.reset_parameters()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "        for epoch in range(1, 1 + args.epochs):\n",
    "\n",
    "            loss = train(model, x, adj, y_true, train_idx, optimizer)\n",
    "\n",
    "            if epoch % args.eval_steps == 0:\n",
    "                result = test(model, x, adj, y_true, split_idx, evaluator)\n",
    "                logger.add_result(run, result)\n",
    "\n",
    "                if epoch % args.log_steps == 0:                \n",
    "                    train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                    print(f'Run: {run + 1:02d}\\t '\n",
    "                          f'Epoch: {epoch:02d}\\t '\n",
    "                          f'Loss: {loss:.4f}\\t '\n",
    "                          f'Train: {100 * train_rocauc:.2f}%\\t '\n",
    "                          f'Valid: {100 * valid_rocauc:.2f}%\\t'\n",
    "                          f'Test: {100 * test_rocauc:.2f}%')\n",
    "                    if(test_rocauc > best_test_score):\n",
    "                        best_test_score = test_rocauc\n",
    "                        save_path = \"gcn.pth\"\n",
    "                        torch.save(model, save_path)\n",
    "                        print(\"Model saved.\")\n",
    "    \n",
    "        logger.print_statistics(run)\n",
    "    logger.print_statistics()\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 01, Epoch: 10, Loss: 0.3713, Train: 65.53%, Valid: 61.15% Test: 55.08%\n",
      "Run: 01, Epoch: 20, Loss: 0.3426, Train: 66.51%, Valid: 60.74% Test: 57.72%\n",
      "Run: 01, Epoch: 30, Loss: 0.3337, Train: 67.72%, Valid: 64.02% Test: 59.50%\n",
      "Run: 01, Epoch: 40, Loss: 0.3268, Train: 69.17%, Valid: 66.50% Test: 62.55%\n",
      "Run: 01, Epoch: 50, Loss: 0.3224, Train: 70.27%, Valid: 67.63% Test: 63.56%\n",
      "Run: 01, Epoch: 60, Loss: 0.3184, Train: 71.51%, Valid: 68.67% Test: 64.29%\n",
      "Run: 01, Epoch: 70, Loss: 0.3147, Train: 73.06%, Valid: 70.08% Test: 65.53%\n",
      "Run: 01, Epoch: 80, Loss: 0.3109, Train: 74.49%, Valid: 71.25% Test: 67.18%\n",
      "Run: 01, Epoch: 90, Loss: 0.3074, Train: 75.55%, Valid: 72.16% Test: 68.06%\n",
      "Run: 01, Epoch: 100, Loss: 0.3047, Train: 76.33%, Valid: 72.87% Test: 68.47%\n",
      "Run: 01, Epoch: 110, Loss: 0.3024, Train: 76.96%, Valid: 73.54% Test: 68.76%\n",
      "Run: 01, Epoch: 120, Loss: 0.3007, Train: 77.37%, Valid: 73.78% Test: 68.70%\n",
      "Run: 01, Epoch: 130, Loss: 0.2991, Train: 77.74%, Valid: 74.04% Test: 69.03%\n",
      "Run: 01, Epoch: 140, Loss: 0.2983, Train: 78.00%, Valid: 74.18% Test: 69.09%\n",
      "Run: 01, Epoch: 150, Loss: 0.2973, Train: 78.22%, Valid: 74.33% Test: 69.23%\n",
      "Run: 01, Epoch: 160, Loss: 0.2964, Train: 78.45%, Valid: 74.53% Test: 69.38%\n",
      "Run: 01, Epoch: 170, Loss: 0.2956, Train: 78.64%, Valid: 74.68% Test: 69.54%\n",
      "Run: 01, Epoch: 180, Loss: 0.2948, Train: 78.83%, Valid: 74.82% Test: 69.68%\n",
      "Run: 01, Epoch: 190, Loss: 0.2944, Train: 78.98%, Valid: 74.89% Test: 69.73%\n",
      "Run: 01, Epoch: 200, Loss: 0.2936, Train: 79.09%, Valid: 75.00% Test: 69.79%\n",
      "Run: 01, Epoch: 210, Loss: 0.2931, Train: 79.25%, Valid: 75.12% Test: 69.84%\n",
      "Run: 01, Epoch: 220, Loss: 0.2927, Train: 79.34%, Valid: 75.25% Test: 69.94%\n",
      "Run: 01, Epoch: 230, Loss: 0.2918, Train: 79.49%, Valid: 75.35% Test: 69.97%\n",
      "Run: 01, Epoch: 240, Loss: 0.2914, Train: 79.62%, Valid: 75.40% Test: 70.03%\n",
      "Run: 01, Epoch: 250, Loss: 0.2913, Train: 79.68%, Valid: 75.48% Test: 70.03%\n",
      "Run: 01, Epoch: 260, Loss: 0.2904, Train: 79.76%, Valid: 75.53% Test: 70.02%\n",
      "Run: 01, Epoch: 270, Loss: 0.2899, Train: 79.86%, Valid: 75.65% Test: 70.14%\n",
      "Run: 01, Epoch: 280, Loss: 0.2897, Train: 79.95%, Valid: 75.70% Test: 70.19%\n",
      "Run: 01, Epoch: 290, Loss: 0.2896, Train: 80.02%, Valid: 75.74% Test: 70.24%\n",
      "Run: 01, Epoch: 300, Loss: 0.2889, Train: 80.04%, Valid: 75.82% Test: 70.35%\n",
      "Run: 01, Epoch: 310, Loss: 0.2887, Train: 80.12%, Valid: 75.85% Test: 70.38%\n",
      "Run: 01, Epoch: 320, Loss: 0.2884, Train: 80.21%, Valid: 75.89% Test: 70.40%\n",
      "Run: 01, Epoch: 330, Loss: 0.2881, Train: 80.23%, Valid: 75.93% Test: 70.44%\n",
      "Run: 01, Epoch: 340, Loss: 0.2873, Train: 80.32%, Valid: 76.02% Test: 70.62%\n",
      "Run: 01, Epoch: 350, Loss: 0.2886, Train: 80.33%, Valid: 76.04% Test: 70.59%\n",
      "Run: 01, Epoch: 360, Loss: 0.2872, Train: 80.43%, Valid: 76.06% Test: 70.63%\n",
      "Run: 01, Epoch: 370, Loss: 0.2868, Train: 80.49%, Valid: 76.12% Test: 70.73%\n",
      "Run: 01, Epoch: 380, Loss: 0.2863, Train: 80.54%, Valid: 76.18% Test: 70.80%\n",
      "Run: 01, Epoch: 390, Loss: 0.2861, Train: 80.59%, Valid: 76.22% Test: 70.83%\n",
      "Run: 01, Epoch: 400, Loss: 0.2859, Train: 80.64%, Valid: 76.22% Test: 70.82%\n",
      "Run: 01, Epoch: 410, Loss: 0.2858, Train: 80.67%, Valid: 76.22% Test: 70.78%\n",
      "Run: 01, Epoch: 420, Loss: 0.2855, Train: 80.70%, Valid: 76.26% Test: 70.86%\n",
      "Run: 01, Epoch: 430, Loss: 0.2853, Train: 80.75%, Valid: 76.32% Test: 70.95%\n",
      "Run: 01, Epoch: 440, Loss: 0.2850, Train: 80.76%, Valid: 76.33% Test: 70.97%\n",
      "Run: 01, Epoch: 450, Loss: 0.2847, Train: 80.83%, Valid: 76.37% Test: 71.03%\n",
      "Run: 01, Epoch: 460, Loss: 0.2850, Train: 80.86%, Valid: 76.35% Test: 70.99%\n",
      "Run: 01, Epoch: 470, Loss: 0.2845, Train: 80.87%, Valid: 76.44% Test: 71.14%\n",
      "Run: 01, Epoch: 480, Loss: 0.2841, Train: 80.94%, Valid: 76.47% Test: 71.18%\n",
      "Run: 01, Epoch: 490, Loss: 0.2845, Train: 80.97%, Valid: 76.47% Test: 71.17%\n",
      "Run: 01, Epoch: 500, Loss: 0.2844, Train: 80.96%, Valid: 76.53% Test: 71.32%\n",
      "Run: 01, Epoch: 510, Loss: 0.2836, Train: 81.04%, Valid: 76.54% Test: 71.34%\n",
      "Run: 01, Epoch: 520, Loss: 0.2835, Train: 81.07%, Valid: 76.54% Test: 71.25%\n",
      "Run: 01, Epoch: 530, Loss: 0.2833, Train: 81.11%, Valid: 76.65% Test: 71.51%\n",
      "Run: 01, Epoch: 540, Loss: 0.2834, Train: 81.10%, Valid: 76.66% Test: 71.61%\n",
      "Run: 01, Epoch: 550, Loss: 0.2829, Train: 81.16%, Valid: 76.66% Test: 71.49%\n",
      "Run: 01, Epoch: 560, Loss: 0.2827, Train: 81.20%, Valid: 76.67% Test: 71.58%\n",
      "Run: 01, Epoch: 570, Loss: 0.2836, Train: 81.22%, Valid: 76.73% Test: 71.69%\n",
      "Run: 01, Epoch: 580, Loss: 0.2825, Train: 81.25%, Valid: 76.68% Test: 71.60%\n",
      "Run: 01, Epoch: 590, Loss: 0.2823, Train: 81.26%, Valid: 76.75% Test: 71.75%\n",
      "Run: 01, Epoch: 600, Loss: 0.2821, Train: 81.31%, Valid: 76.77% Test: 71.80%\n",
      "Run: 01, Epoch: 610, Loss: 0.2819, Train: 81.34%, Valid: 76.84% Test: 71.99%\n",
      "Run: 01, Epoch: 620, Loss: 0.2832, Train: 81.28%, Valid: 76.79% Test: 71.90%\n",
      "Run: 01, Epoch: 630, Loss: 0.2815, Train: 81.36%, Valid: 76.82% Test: 72.01%\n",
      "Run: 01, Epoch: 640, Loss: 0.2818, Train: 81.41%, Valid: 76.86% Test: 72.07%\n",
      "Run: 01, Epoch: 650, Loss: 0.2814, Train: 81.45%, Valid: 76.90% Test: 72.12%\n",
      "Run: 01, Epoch: 660, Loss: 0.2813, Train: 81.47%, Valid: 76.92% Test: 72.16%\n",
      "Run: 01, Epoch: 670, Loss: 0.2811, Train: 81.49%, Valid: 76.91% Test: 72.17%\n",
      "Run: 01, Epoch: 680, Loss: 0.2808, Train: 81.52%, Valid: 76.93% Test: 72.20%\n",
      "Run: 01, Epoch: 690, Loss: 0.2815, Train: 81.54%, Valid: 76.96% Test: 72.24%\n",
      "Run: 01, Epoch: 700, Loss: 0.2812, Train: 81.53%, Valid: 76.95% Test: 72.31%\n",
      "Run: 01, Epoch: 710, Loss: 0.2806, Train: 81.60%, Valid: 77.04% Test: 72.35%\n",
      "Run: 01, Epoch: 720, Loss: 0.2806, Train: 81.59%, Valid: 77.02% Test: 72.37%\n",
      "Run: 01, Epoch: 730, Loss: 0.2804, Train: 81.65%, Valid: 77.07% Test: 72.48%\n",
      "Run: 01, Epoch: 740, Loss: 0.2801, Train: 81.67%, Valid: 77.06% Test: 72.45%\n",
      "Run: 01, Epoch: 750, Loss: 0.2812, Train: 81.67%, Valid: 77.02% Test: 72.39%\n",
      "Run: 01, Epoch: 760, Loss: 0.2801, Train: 81.71%, Valid: 77.06% Test: 72.47%\n",
      "Run: 01, Epoch: 770, Loss: 0.2797, Train: 81.71%, Valid: 77.13% Test: 72.53%\n",
      "Run: 01, Epoch: 780, Loss: 0.2798, Train: 81.75%, Valid: 77.11% Test: 72.56%\n",
      "Run: 01, Epoch: 790, Loss: 0.2793, Train: 81.77%, Valid: 77.16% Test: 72.59%\n",
      "Run: 01, Epoch: 800, Loss: 0.2796, Train: 81.79%, Valid: 77.13% Test: 72.54%\n",
      "Run: 01, Epoch: 810, Loss: 0.2792, Train: 81.81%, Valid: 77.16% Test: 72.55%\n",
      "Run: 01, Epoch: 820, Loss: 0.2797, Train: 81.79%, Valid: 77.14% Test: 72.53%\n",
      "Run: 01, Epoch: 830, Loss: 0.2791, Train: 81.83%, Valid: 77.15% Test: 72.55%\n",
      "Run: 01, Epoch: 840, Loss: 0.2792, Train: 81.88%, Valid: 77.21% Test: 72.65%\n",
      "Run: 01, Epoch: 850, Loss: 0.2788, Train: 81.88%, Valid: 77.17% Test: 72.55%\n",
      "Run: 01, Epoch: 860, Loss: 0.2788, Train: 81.92%, Valid: 77.23% Test: 72.66%\n",
      "Run: 01, Epoch: 870, Loss: 0.2792, Train: 81.88%, Valid: 77.21% Test: 72.63%\n",
      "Run: 01, Epoch: 880, Loss: 0.2786, Train: 81.95%, Valid: 77.23% Test: 72.64%\n",
      "Run: 01, Epoch: 890, Loss: 0.2785, Train: 81.96%, Valid: 77.27% Test: 72.73%\n",
      "Run: 01, Epoch: 900, Loss: 0.2786, Train: 81.95%, Valid: 77.27% Test: 72.71%\n",
      "Run: 01, Epoch: 910, Loss: 0.2781, Train: 82.01%, Valid: 77.28% Test: 72.75%\n",
      "Run: 01, Epoch: 920, Loss: 0.2784, Train: 82.04%, Valid: 77.32% Test: 72.78%\n",
      "Run: 01, Epoch: 930, Loss: 0.2781, Train: 82.02%, Valid: 77.27% Test: 72.68%\n",
      "Run: 01, Epoch: 940, Loss: 0.2777, Train: 82.06%, Valid: 77.29% Test: 72.74%\n",
      "Run: 01, Epoch: 950, Loss: 0.2776, Train: 82.10%, Valid: 77.33% Test: 72.72%\n",
      "Run: 01, Epoch: 960, Loss: 0.2776, Train: 82.13%, Valid: 77.35% Test: 72.84%\n",
      "Run: 01, Epoch: 970, Loss: 0.2776, Train: 82.09%, Valid: 77.30% Test: 72.71%\n",
      "Run: 01, Epoch: 980, Loss: 0.2779, Train: 82.11%, Valid: 77.31% Test: 72.76%\n",
      "Run: 01, Epoch: 990, Loss: 0.2772, Train: 82.19%, Valid: 77.40% Test: 72.88%\n",
      "Run: 01, Epoch: 1000, Loss: 0.2771, Train: 82.22%, Valid: 77.42% Test: 72.92%\n",
      "Run 01:\n",
      "Highest Train: 82.22\n",
      "Highest Valid: 77.42\n",
      "  Final Train: 82.22\n",
      "   Final Test: 72.92\n",
      "All runs:\n",
      "Highest Train: 82.22 ± nan\n",
      "Highest Valid: 77.42 ± nan\n",
      "  Final Train: 82.22 ± nan\n",
      "   Final Test: 72.92 ± nan\n"
     ]
    }
   ],
   "source": [
    "def train(model, x, y_true, train_idx, optimizer):\n",
    "    model.train()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)[train_idx]\n",
    "    loss = criterion(out, y_true[train_idx].to(torch.float))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, x, y_true, split_idx, evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred = model(x)\n",
    "\n",
    "    train_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['train']],\n",
    "        'y_pred': y_pred[split_idx['train']],\n",
    "    })['rocauc']\n",
    "    valid_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['valid']],\n",
    "        'y_pred': y_pred[split_idx['valid']],\n",
    "    })['rocauc']\n",
    "    test_rocauc = evaluator.eval({\n",
    "        'y_true': y_true[split_idx['test']],\n",
    "        'y_pred': y_pred[split_idx['test']],\n",
    "    })['rocauc']\n",
    "\n",
    "    return train_rocauc, valid_rocauc, test_rocauc\n",
    "\n",
    "\n",
    "model = MLP(x.size(-1), args.hidden_channels, 112, args.num_layers,\n",
    "                args.dropout).to(device)\n",
    "\n",
    "evaluator = Evaluator(name='ogbn-proteins')\n",
    "logger = Logger(args.runs, args)\n",
    "\n",
    "for run in range(args.runs):\n",
    "    model.reset_parameters()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    for epoch in range(1, 1 + args.epochs):\n",
    "        loss = train(model, x, y_true, train_idx, optimizer)\n",
    "\n",
    "        if epoch % args.eval_steps == 0:\n",
    "            result = test(model, x, y_true, split_idx, evaluator)\n",
    "            logger.add_result(run, result)\n",
    "\n",
    "            if epoch % args.log_steps == 0:\n",
    "                train_rocauc, valid_rocauc, test_rocauc = result\n",
    "                print(f'Run: {run + 1:02d}, '\n",
    "                      f'Epoch: {epoch:02d}, '\n",
    "                      f'Loss: {loss:.4f}, '\n",
    "                      f'Train: {100 * train_rocauc:.2f}%, '\n",
    "                      f'Valid: {100 * valid_rocauc:.2f}% '\n",
    "                      f'Test: {100 * test_rocauc:.2f}%')\n",
    "\n",
    "    logger.print_statistics(run)\n",
    "logger.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 minutes\n"
     ]
    }
   ],
   "source": [
    "print(humanize.naturaldelta(start_time - time.time()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
